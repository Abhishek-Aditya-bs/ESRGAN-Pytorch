{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee94cd9",
   "metadata": {
    "papermill": {
     "duration": 0.016914,
     "end_time": "2022-02-17T17:40:14.881571",
     "exception": false,
     "start_time": "2022-02-17T17:40:14.864657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Prepration files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926acb55",
   "metadata": {
    "papermill": {
     "duration": 0.01565,
     "end_time": "2022-02-17T17:40:14.913226",
     "exception": false,
     "start_time": "2022-02-17T17:40:14.897576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## imgproc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf663401",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T17:40:15.082391Z",
     "iopub.status.busy": "2022-02-17T17:40:14.950071Z",
     "iopub.status.idle": "2022-02-17T17:40:16.840828Z",
     "shell.execute_reply": "2022-02-17T17:40:16.839975Z"
    },
    "papermill": {
     "duration": 1.911163,
     "end_time": "2022-02-17T17:40:16.840992",
     "exception": false,
     "start_time": "2022-02-17T17:40:14.929829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def normalize(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize the ``OpenCV.imread`` or ``skimage.io.imread`` data.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The image data read by ``OpenCV.imread`` or ``skimage.io.imread``.\n",
    "\n",
    "    Returns:\n",
    "        Normalized image data. Data range [0, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    return image.astype(np.float64) / 255.0\n",
    "\n",
    "\n",
    "def unnormalize(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Un-normalize the ``OpenCV.imread`` or ``skimage.io.imread`` data.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The image data read by ``OpenCV.imread`` or ``skimage.io.imread``.\n",
    "\n",
    "    Returns:\n",
    "        Denormalized image data. Data range [0, 255].\n",
    "    \"\"\"\n",
    "\n",
    "    return image.astype(np.float64) * 255.0\n",
    "\n",
    "\n",
    "def image2tensor(image: np.ndarray, range_norm: bool, half: bool) -> torch.Tensor:\n",
    "    \"\"\"Convert ``PIL.Image`` to Tensor.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The image data read by ``PIL.Image``\n",
    "        range_norm (bool): Scale [0, 1] data to between [-1, 1]\n",
    "        half (bool): Whether to convert torch.float32 similarly to torch.half type.\n",
    "\n",
    "    Returns:\n",
    "        Normalized image data\n",
    "\n",
    "    Examples:\n",
    "        >>> image = Image.open(\"image.bmp\")\n",
    "        >>> tensor_image = image2tensor(image, range_norm=False, half=False)\n",
    "    \"\"\"\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    tensor = convert_tensor(image)\n",
    "\n",
    "    if range_norm:\n",
    "        tensor = tensor.mul_(2.0).sub_(1.0)\n",
    "    if half:\n",
    "        tensor = tensor.half()\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor2image(tensor: torch.Tensor, range_norm: bool, half: bool) -> Any:\n",
    "    \"\"\"Converts ``torch.Tensor`` to ``PIL.Image``.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): The image that needs to be converted to ``PIL.Image``\n",
    "        range_norm (bool): Scale [-1, 1] data to between [0, 1]\n",
    "        half (bool): Whether to convert torch.float32 similarly to torch.half type.\n",
    "\n",
    "    Returns:\n",
    "        Convert image data to support PIL library\n",
    "\n",
    "    Examples:\n",
    "        >>> tensor = torch.randn([1, 3, 128, 128])\n",
    "        >>> image = tensor2image(tensor, range_norm=False, half=False)\n",
    "    \"\"\"\n",
    "\n",
    "    if range_norm:\n",
    "        tensor = tensor.add_(1.0).div_(2.0)\n",
    "    if half:\n",
    "        tensor = tensor.half()\n",
    "\n",
    "    image = tensor.squeeze_(0).permute(1, 2, 0).mul_(255).clamp_(0, 255).cpu().numpy().astype(\"uint8\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def convert_rgb_to_y(image: Any) -> Any:\n",
    "    \"\"\"Convert RGB image or tensor image data to YCbCr(Y) format.\n",
    "\n",
    "    Args:\n",
    "        image: RGB image data read by ``PIL.Image''.\n",
    "\n",
    "    Returns:\n",
    "        Y image array data.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(image) == np.ndarray:\n",
    "        return 16. + (64.738 * image[:, :, 0] + 129.057 * image[:, :, 1] + 25.064 * image[:, :, 2]) / 256.\n",
    "    elif type(image) == torch.Tensor:\n",
    "        if len(image.shape) == 4:\n",
    "            image = image.squeeze_(0)\n",
    "        return 16. + (64.738 * image[0, :, :] + 129.057 * image[1, :, :] + 25.064 * image[2, :, :]) / 256.\n",
    "    else:\n",
    "        raise Exception(\"Unknown Type\", type(image))\n",
    "\n",
    "\n",
    "def convert_rgb_to_ycbcr(image: Any) -> Any:\n",
    "    \"\"\"Convert RGB image or tensor image data to YCbCr format.\n",
    "\n",
    "    Args:\n",
    "        image: RGB image data read by ``PIL.Image''.\n",
    "\n",
    "    Returns:\n",
    "        YCbCr image array data.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(image) == np.ndarray:\n",
    "        y = 16. + (64.738 * image[:, :, 0] + 129.057 * image[:, :, 1] + 25.064 * image[:, :, 2]) / 256.\n",
    "        cb = 128. + (-37.945 * image[:, :, 0] - 74.494 * image[:, :, 1] + 112.439 * image[:, :, 2]) / 256.\n",
    "        cr = 128. + (112.439 * image[:, :, 0] - 94.154 * image[:, :, 1] - 18.285 * image[:, :, 2]) / 256.\n",
    "        return np.array([y, cb, cr]).transpose([1, 2, 0])\n",
    "    elif type(image) == torch.Tensor:\n",
    "        if len(image.shape) == 4:\n",
    "            image = image.squeeze(0)\n",
    "        y = 16. + (64.738 * image[0, :, :] + 129.057 * image[1, :, :] + 25.064 * image[2, :, :]) / 256.\n",
    "        cb = 128. + (-37.945 * image[0, :, :] - 74.494 * image[1, :, :] + 112.439 * image[2, :, :]) / 256.\n",
    "        cr = 128. + (112.439 * image[0, :, :] - 94.154 * image[1, :, :] - 18.285 * image[2, :, :]) / 256.\n",
    "        return torch.cat([y, cb, cr], 0).permute(1, 2, 0)\n",
    "    else:\n",
    "        raise Exception(\"Unknown Type\", type(image))\n",
    "\n",
    "\n",
    "def convert_ycbcr_to_rgb(image: Any) -> Any:\n",
    "    \"\"\"Convert YCbCr format image to RGB format.\n",
    "\n",
    "    Args:\n",
    "       image: YCbCr image data read by ``PIL.Image''.\n",
    "\n",
    "    Returns:\n",
    "        RGB image array data.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(image) == np.ndarray:\n",
    "        r = 298.082 * image[:, :, 0] / 256. + 408.583 * image[:, :, 2] / 256. - 222.921\n",
    "        g = 298.082 * image[:, :, 0] / 256. - 100.291 * image[:, :, 1] / 256. - 208.120 * image[:, :, 2] / 256. + 135.576\n",
    "        b = 298.082 * image[:, :, 0] / 256. + 516.412 * image[:, :, 1] / 256. - 276.836\n",
    "        return np.array([r, g, b]).transpose([1, 2, 0])\n",
    "    elif type(image) == torch.Tensor:\n",
    "        if len(image.shape) == 4:\n",
    "            image = image.squeeze(0)\n",
    "        r = 298.082 * image[0, :, :] / 256. + 408.583 * image[2, :, :] / 256. - 222.921\n",
    "        g = 298.082 * image[0, :, :] / 256. - 100.291 * image[1, :, :] / 256. - 208.120 * image[2, :, :] / 256. + 135.576\n",
    "        b = 298.082 * image[0, :, :] / 256. + 516.412 * image[1, :, :] / 256. - 276.836\n",
    "        return torch.cat([r, g, b], 0).permute(1, 2, 0)\n",
    "    else:\n",
    "        raise Exception(\"Unknown Type\", type(image))\n",
    "\n",
    "\n",
    "def center_crop(lr: Any, hr: Any, image_size: int, upscale_factor: int):\n",
    "    \"\"\"Cut ``PIL.Image`` in the center area of the image.\n",
    "\n",
    "    Args:\n",
    "        lr: Low-resolution image data read by ``PIL.Image``.\n",
    "        hr: High-resolution image data read by ``PIL.Image``.\n",
    "        image_size (int): The size of the captured image area. It should be the size of the high-resolution image.\n",
    "        upscale_factor (int): magnification factor.\n",
    "\n",
    "    Returns:\n",
    "        Randomly cropped low-resolution images and high-resolution images.\n",
    "    \"\"\"\n",
    "\n",
    "    w, h = hr.size\n",
    "\n",
    "    left = (w - image_size) // 2\n",
    "    top = (h - image_size) // 2\n",
    "    right = left + image_size\n",
    "    bottom = top + image_size\n",
    "\n",
    "    lr = lr.crop((left // upscale_factor,\n",
    "                  top // upscale_factor,\n",
    "                  right // upscale_factor,\n",
    "                  bottom // upscale_factor))\n",
    "    hr = hr.crop((left, top, right, bottom))\n",
    "\n",
    "    return lr, hr\n",
    "\n",
    "\n",
    "def random_crop(lr: Any, hr: Any, image_size: int, upscale_factor: int):\n",
    "    \"\"\"Will ``PIL.Image`` randomly capture the specified area of the image.\n",
    "\n",
    "    Args:\n",
    "        lr: Low-resolution image data read by ``PIL.Image``.\n",
    "        hr: High-resolution image data read by ``PIL.Image``.\n",
    "        image_size (int): The size of the captured image area. It should be the size of the high-resolution image.\n",
    "        upscale_factor (int): magnification factor.\n",
    "\n",
    "    Returns:\n",
    "        Randomly cropped low-resolution images and high-resolution images.\n",
    "    \"\"\"\n",
    "\n",
    "    w, h = hr.size\n",
    "    left = torch.randint(0, w - image_size + 1, size=(1,)).item()\n",
    "    top = torch.randint(0, h - image_size + 1, size=(1,)).item()\n",
    "    right = left + image_size\n",
    "    bottom = top + image_size\n",
    "\n",
    "    lr = lr.crop((left // upscale_factor,\n",
    "                  top // upscale_factor,\n",
    "                  right // upscale_factor,\n",
    "                  bottom // upscale_factor))\n",
    "    hr = hr.crop((left, top, right, bottom))\n",
    "\n",
    "    return lr, hr\n",
    "\n",
    "\n",
    "def random_rotate(lr: Any, hr: Any, degrees: list):\n",
    "    \"\"\"Will ``PIL.Image`` randomly rotate the image.\n",
    "\n",
    "    Args:\n",
    "        lr: Low-resolution image data read by ``PIL.Image``.\n",
    "        hr: High-resolution image data read by ``PIL.Image``.\n",
    "        degrees (list): rotation angle, clockwise and counterclockwise rotation.\n",
    "\n",
    "    Returns:\n",
    "        Randomly rotated low-resolution images and high-resolution images.\n",
    "    \"\"\"\n",
    "\n",
    "    angle = random.choice(degrees)\n",
    "    lr = F.rotate(lr, angle)\n",
    "    hr = F.rotate(hr, angle)\n",
    "\n",
    "    return lr, hr\n",
    "\n",
    "\n",
    "def random_horizontally_flip(lr: Any, hr: Any, p=0.5):\n",
    "    \"\"\"Flip the ``PIL.Image`` image horizontally randomly.\n",
    "\n",
    "    Args:\n",
    "        lr: Low-resolution image data read by ``PIL.Image``.\n",
    "        hr: High-resolution image data read by ``PIL.Image``.\n",
    "        p (optional, float): rollover probability. (Default: 0.5)\n",
    "\n",
    "    Returns:\n",
    "        Low-resolution image and high-resolution image after random horizontal flip.\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.rand(1).item() > p:\n",
    "        lr = F.hflip(lr)\n",
    "        hr = F.hflip(hr)\n",
    "\n",
    "    return lr, hr\n",
    "\n",
    "\n",
    "def random_vertically_flip(lr: Any, hr: Any, p=0.5):\n",
    "    \"\"\"Turn the ``PIL.Image`` image upside down randomly.\n",
    "\n",
    "    Args:\n",
    "        lr: Low-resolution image data read by ``PIL.Image``.\n",
    "        hr: High-resolution image data read by ``PIL.Image``.\n",
    "        p (optional, float): rollover probability. (Default: 0.5)\n",
    "\n",
    "    Returns:\n",
    "        Randomly rotated up and down low-resolution images and high-resolution images.\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.rand(1).item() > p:\n",
    "        lr = F.vflip(lr)\n",
    "        hr = F.vflip(hr)\n",
    "\n",
    "    return lr, hr\n",
    "\n",
    "\n",
    "def random_adjust_brightness(lr: Any, hr: Any, factor: list):\n",
    "    \"\"\"Set ``PIL.Image`` to randomly adjust the image brightness.\n",
    "\n",
    "    Args:\n",
    "        lr: Low-resolution image data read by ``PIL.Image``.\n",
    "        hr: High-resolution image data read by ``PIL.Image``.\n",
    "        factor (list): Brightness coefficient adjustment range.\n",
    "\n",
    "    Returns:\n",
    "        Low-resolution image and high-resolution image with randomly adjusted brightness.\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly adjust the brightness gain range.\n",
    "    brightness_factor = random.choice(factor)\n",
    "    lr = F.adjust_brightness(lr, brightness_factor)\n",
    "    hr = F.adjust_brightness(hr, brightness_factor)\n",
    "\n",
    "    return lr, hr\n",
    "\n",
    "\n",
    "def random_adjust_contrast(lr: Any, hr: Any, factor: list):\n",
    "    \"\"\"Set ``PIL.Image`` to randomly adjust the image contrast.\n",
    "\n",
    "    Args:\n",
    "        lr: Low-resolution image data read by ``PIL.Image``.\n",
    "        hr: High-resolution image data read by ``PIL.Image``.\n",
    "        factor (list): Contrast coefficient adjustment range.\n",
    "\n",
    "    Returns:\n",
    "        Low-resolution image and high-resolution image with randomly adjusted contrast.\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly adjust the contrast gain range.\n",
    "    contrast_factor = random.choice(factor)\n",
    "    lr = F.adjust_contrast(lr, contrast_factor)\n",
    "    hr = F.adjust_contrast(hr, contrast_factor)\n",
    "\n",
    "    return lr, hr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb4ae86",
   "metadata": {
    "papermill": {
     "duration": 0.01585,
     "end_time": "2022-02-17T17:40:16.873706",
     "exception": false,
     "start_time": "2022-02-17T17:40:16.857856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9979d173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T17:40:16.928952Z",
     "iopub.status.busy": "2022-02-17T17:40:16.927719Z",
     "iopub.status.idle": "2022-02-17T17:40:16.937175Z",
     "shell.execute_reply": "2022-02-17T17:40:16.936682Z"
    },
    "papermill": {
     "duration": 0.047599,
     "end_time": "2022-02-17T17:40:16.937314",
     "exception": false,
     "start_time": "2022-02-17T17:40:16.889715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import lmdb\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode as IMode\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Customize the data set loading function and prepare low/high resolution image data in advance.\n",
    "\n",
    "    Args:\n",
    "        dataroot         (str): Training data set address\n",
    "        image_size       (int): High resolution image size\n",
    "        upscale_factor   (int): Image magnification\n",
    "        mode             (str): Data set loading method, the training data set is for data enhancement,\n",
    "                             and the verification data set is not for data enhancement\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataroot: str, image_size: int, upscale_factor: int, mode: str) -> None:\n",
    "        super(ImageDataset, self).__init__()\n",
    "        self.filenames = [os.path.join(dataroot, x) for x in os.listdir(dataroot)]\n",
    "\n",
    "        if mode == \"train\":\n",
    "            self.hr_transforms = transforms.Compose([\n",
    "                transforms.RandomCrop(image_size),\n",
    "                transforms.RandomRotation([0, 90]),\n",
    "                transforms.RandomHorizontalFlip(0.5),\n",
    "            ])\n",
    "        elif mode == \"valid\":\n",
    "            self.hr_transforms = transforms.CenterCrop(image_size)\n",
    "        else:\n",
    "            raise \"Unsupported data processing model, please use `train` or `valid`.\"\n",
    "\n",
    "        self.lr_transforms = transforms.Resize(image_size // upscale_factor, interpolation=IMode.BICUBIC)\n",
    "\n",
    "    def __getitem__(self, batch_index: int):\n",
    "        # Read a batch of image data\n",
    "        image = Image.open(self.filenames[batch_index])\n",
    "\n",
    "        # Transform image\n",
    "        hr_image = self.hr_transforms(image)\n",
    "        lr_image = self.lr_transforms(hr_image)\n",
    "\n",
    "        # Convert image data into Tensor stream format (PyTorch).\n",
    "        # Note: The range of input and output is between [0, 1]\n",
    "        lr_tensor = image2tensor(lr_image, range_norm=False, half=False)\n",
    "        hr_tensor = image2tensor(hr_image, range_norm=False, half=False)\n",
    "\n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)\n",
    "\n",
    "\n",
    "class LMDBDataset(Dataset):\n",
    "    \"\"\"Load the data set as a data set in the form of LMDB.\n",
    "\n",
    "    Attributes:\n",
    "        lr_datasets (list): Low-resolution image data in the dataset\n",
    "        hr_datasets (list): High-resolution image data in the dataset\n",
    "\n",
    "    Args:\n",
    "        lr_lmdb_path (str): LMDB file address of low-resolution image\n",
    "        hr_lmdb_path (int): LMDB file address of high-resolution image\n",
    "        image_size (int): High resolution image size\n",
    "        upscale_factor (int): Image magnification\n",
    "        mode (str): Data set loading method, the training data set is for data enhancement,\n",
    "            and the verification data set is not for data enhancement\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr_lmdb_path: str, hr_lmdb_path: str, image_size: int, upscale_factor: int, mode: str) -> None:\n",
    "        super(LMDBDataset, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.mode = mode\n",
    "\n",
    "        # Create low/high resolution image array\n",
    "        self.lr_datasets = []\n",
    "        self.hr_datasets = []\n",
    "\n",
    "        # Initialize the LMDB database file address\n",
    "        self.lr_lmdb_path = lr_lmdb_path\n",
    "        self.hr_lmdb_path = hr_lmdb_path\n",
    "\n",
    "        # Write image data in LMDB database to memory\n",
    "        self.read_lmdb_dataset()\n",
    "\n",
    "    def __getitem__(self, batch_index: int):\n",
    "        # Read a batch of image data\n",
    "        lr_image = self.lr_datasets[batch_index]\n",
    "        hr_image = self.hr_datasets[batch_index]\n",
    "\n",
    "        # Data augment\n",
    "        if self.mode == \"train:\":\n",
    "            lr_image, hr_image = random_crop(lr_image, hr_image, image_size=self.image_size, upscale_factor=self.upscale_factor)\n",
    "            lr_image, hr_image = random_rotate(lr_image, hr_image, degrees=[0, 90])\n",
    "            lr_image, hr_image = random_horizontally_flip(lr_image, hr_image, p=0.5)\n",
    "        elif self.mode == \"valid:\":\n",
    "            lr_image, hr_image = center_crop(lr_image, hr_image, image_size=self.image_size, upscale_factor=self.upscale_factor)\n",
    "        else:\n",
    "            raise \"Unsupported data processing model, please use `train` or `valid`.\"\n",
    "\n",
    "        # Convert image data into Tensor stream format (PyTorch).\n",
    "        # Note: The range of input and output is between [0, 1]\n",
    "        lr_tensor = image2tensor(lr_image, range_norm=False, half=False)\n",
    "        hr_tensor = image2tensor(hr_image, range_norm=False, half=False)\n",
    "\n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.hr_datasets)\n",
    "\n",
    "    def read_lmdb_dataset(self):\n",
    "        # Open two LMDB database writing environments to read low/high image data\n",
    "        lr_lmdb_env = lmdb.open(self.lr_lmdb_path)\n",
    "        hr_lmdb_env = lmdb.open(self.hr_lmdb_path)\n",
    "\n",
    "        # Write the image data in the low-resolution LMDB data set to the memory\n",
    "        for _, image_bytes in lr_lmdb_env.begin().cursor():\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            self.lr_datasets.append(image)\n",
    "\n",
    "        # Write the image data in the high-resolution LMDB data set to the memory\n",
    "        for _, image_bytes in hr_lmdb_env.begin().cursor():\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            self.hr_datasets.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6beccec",
   "metadata": {
    "papermill": {
     "duration": 0.016095,
     "end_time": "2022-02-17T17:40:16.969434",
     "exception": false,
     "start_time": "2022-02-17T17:40:16.953339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91007a",
   "metadata": {
    "papermill": {
     "duration": 0.015389,
     "end_time": "2022-02-17T17:40:17.000764",
     "exception": false,
     "start_time": "2022-02-17T17:40:16.985375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39196c62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T17:40:17.080155Z",
     "iopub.status.busy": "2022-02-17T17:40:17.066654Z",
     "iopub.status.idle": "2022-02-17T17:40:17.083755Z",
     "shell.execute_reply": "2022-02-17T17:40:17.083285Z"
    },
    "papermill": {
     "duration": 0.067069,
     "end_time": "2022-02-17T17:40:17.083882",
     "exception": false,
     "start_time": "2022-02-17T17:40:17.016813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResidualDenseBlock(nn.Module):\n",
    "    \"\"\"Achieves densely connected convolutional layers.\n",
    "    `Densely Connected Convolutional Networks <https://arxiv.org/pdf/1608.06993v5.pdf>` paper.\n",
    "\n",
    "    Args:\n",
    "        channels (int): The number of channels in the input image.\n",
    "        growths (int): The number of channels that increase in each layer of convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, growths: int) -> None:\n",
    "        super(ResidualDenseBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + growths * 0, growths, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv2 = nn.Conv2d(channels + growths * 1, growths, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(channels + growths * 2, growths, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(channels + growths * 3, growths, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv5 = nn.Conv2d(channels + growths * 4, channels, (3, 3), (1, 1), (1, 1))\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, True)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out1 = self.leaky_relu(self.conv1(x))\n",
    "        out2 = self.leaky_relu(self.conv2(torch.cat([x, out1], 1)))\n",
    "        out3 = self.leaky_relu(self.conv3(torch.cat([x, out1, out2], 1)))\n",
    "        out4 = self.leaky_relu(self.conv4(torch.cat([x, out1, out2, out3], 1)))\n",
    "        out5 = self.identity(self.conv5(torch.cat([x, out1, out2, out3, out4], 1)))\n",
    "        out = torch.mul(out5, 0.2)\n",
    "        out = torch.add(out, identity)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualResidualDenseBlock(nn.Module):\n",
    "    \"\"\"Multi-layer residual dense convolution block.\n",
    "\n",
    "    Args:\n",
    "        channels (int): The number of channels in the input image.\n",
    "        growths (int): The number of channels that increase in each layer of convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, growths: int) -> None:\n",
    "        super(ResidualResidualDenseBlock, self).__init__()\n",
    "        self.rdb1 = ResidualDenseBlock(channels, growths)\n",
    "        self.rdb2 = ResidualDenseBlock(channels, growths)\n",
    "        self.rdb3 = ResidualDenseBlock(channels, growths)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.rdb1(x)\n",
    "        out = self.rdb2(out)\n",
    "        out = self.rdb3(out)\n",
    "        out = torch.mul(out, 0.2)\n",
    "        out = torch.add(out, identity)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # input size. (3) x 128 x 128\n",
    "            nn.Conv2d(3, 64, (3, 3), (1, 1), (1, 1), bias=True),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size. (64) x 64 x 64\n",
    "            nn.Conv2d(64, 64, (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, 128, (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size. (128) x 32 x 32\n",
    "            nn.Conv2d(128, 128, (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(128, 256, (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size. (256) x 16 x 16\n",
    "            nn.Conv2d(256, 256, (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(256, 512, (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size. (512) x 8 x 8\n",
    "            nn.Conv2d(512, 512, (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(512, 512, (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size. (512) x 4 x 4\n",
    "            nn.Conv2d(512, 512, (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 100),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.features(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        # The first layer of convolutional layer.\n",
    "        self.conv1 = nn.Conv2d(3, 64, (3, 3), (1, 1), (1, 1))\n",
    "\n",
    "        # Feature extraction backbone network.\n",
    "        trunk = []\n",
    "        for _ in range(23):\n",
    "            trunk.append(ResidualResidualDenseBlock(64, 32))\n",
    "        self.trunk = nn.Sequential(*trunk)\n",
    "\n",
    "        # After the feature extraction network, reconnect a layer of convolutional blocks.\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "\n",
    "        # Upsampling convolutional layer.\n",
    "        self.upsampling = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "        # Reconnect a layer of convolution block after upsampling.\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "        # Output layer.\n",
    "        self.conv4 = nn.Conv2d(64, 3, (3, 3), (1, 1), (1, 1))\n",
    "\n",
    "    # The model should be defined in the Torch.script method.\n",
    "    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out1 = self.conv1(x)\n",
    "        out = self.trunk(out1)\n",
    "        out2 = self.conv2(out)\n",
    "        out = torch.add(out1, out2)\n",
    "        out = self.upsampling(F.interpolate(out, scale_factor=2, mode=\"nearest\"))\n",
    "        out = self.upsampling(F.interpolate(out, scale_factor=2, mode=\"nearest\"))\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                module.weight.data *= 0.1\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    \"\"\"Constructs a content loss function based on the VGG19 network.\n",
    "    Using high-level feature mapping layers from the latter layers will focus more on the texture content of the image.\n",
    "\n",
    "    Paper reference list:\n",
    "        -`Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network <https://arxiv.org/pdf/1609.04802.pdf>` paper.\n",
    "        -`ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks                    <https://arxiv.org/pdf/1809.00219.pdf>` paper.\n",
    "        -`Perceptual Extreme Super Resolution Network with Receptive Field Block               <https://arxiv.org/pdf/2005.12597.pdf>` paper.\n",
    "\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # Load the VGG19 model trained on the ImageNet dataset.\n",
    "        vgg19 = models.vgg19(pretrained=True).eval()\n",
    "        # Extract the thirty-sixth layer output in the VGG19 model as the content loss.\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg19.features.children())[:35])\n",
    "        # Freeze model parameters.\n",
    "        for parameters in self.feature_extractor.parameters():\n",
    "            parameters.requires_grad = False\n",
    "\n",
    "        # The preprocessing method of the input data. This is the VGG model preprocessing method of the ImageNet dataset.\n",
    "        self.register_buffer(\"mean\", torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"std\", torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, sr: torch.Tensor, hr: torch.Tensor) -> torch.Tensor:\n",
    "        # Standardized operations\n",
    "        sr = sr.sub(self.mean).div(self.std)\n",
    "        hr = hr.sub(self.mean).div(self.std)\n",
    "\n",
    "        # Find the feature map difference between the two images\n",
    "        loss = F.l1_loss(self.feature_extractor(sr), self.feature_extractor(hr))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c61b5",
   "metadata": {
    "papermill": {
     "duration": 0.016079,
     "end_time": "2022-02-17T17:40:17.122883",
     "exception": false,
     "start_time": "2022-02-17T17:40:17.106804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8524b6f",
   "metadata": {
    "papermill": {
     "duration": 0.015503,
     "end_time": "2022-02-17T17:40:17.155161",
     "exception": false,
     "start_time": "2022-02-17T17:40:17.139658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6522dedd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T17:40:17.201151Z",
     "iopub.status.busy": "2022-02-17T17:40:17.200382Z",
     "iopub.status.idle": "2022-02-17T17:40:17.253724Z",
     "shell.execute_reply": "2022-02-17T17:40:17.252802Z"
    },
    "papermill": {
     "duration": 0.081894,
     "end_time": "2022-02-17T17:40:17.253864",
     "exception": false,
     "start_time": "2022-02-17T17:40:17.171970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.backends import cudnn\n",
    "# Random seed to maintain reproducible results\n",
    "torch.manual_seed(0)\n",
    "# Use GPU for training by default\n",
    "if torch.cuda.is_available():\n",
    "    device =  torch.device('cuda')\n",
    "else:\n",
    "    device =  torch.device('cpu')\n",
    "# device = torch.device(\"cuda\", 0)\n",
    "# Turning on when the image size does not change during training can speed up training\n",
    "cudnn.benchmark = True\n",
    "# Image magnification factor\n",
    "upscale_factor = 4\n",
    "# Current configuration parameter method\n",
    "mode = \"train_rrdbnet\"\n",
    "# Experiment name, easy to save weights and log files\n",
    "exp_name = \"RRDBNet_baseline\"\n",
    "\n",
    "if mode == \"train_rrdbnet\":\n",
    "    # Dataset address\n",
    "    train_image_dir = '/kaggle/input/dataset/data/DIV2K/ESRGAN/train/'\n",
    "    valid_image_dir = '/kaggle/input/dataset/data/DIV2K/ESRGAN/valid/'\n",
    "\n",
    "    image_size = 192\n",
    "    batch_size = 48\n",
    "    num_workers = 4\n",
    "\n",
    "    # Incremental training and migration training\n",
    "    resume = False\n",
    "    strict = False\n",
    "    start_epoch = 0\n",
    "    resume_weight = \"\"\n",
    "\n",
    "    # Total num epochs\n",
    "    epochs = 120\n",
    "\n",
    "    # Adam optimizer parameter for RRDBNet(p)\n",
    "    model_lr = 2e-4\n",
    "    model_betas = (0.9, 0.999)\n",
    "\n",
    "    # StepLR scheduler\n",
    "    step_size = epochs // 5\n",
    "    gamma = 0.5\n",
    "\n",
    "    # Print the training log every one hundred iterations\n",
    "    print_frequency = 1000\n",
    "\n",
    "if mode == \"train_esrgan\":\n",
    "    # Dataset address\n",
    "    train_image_dir = 'data/DIV2K/ESRGAN/train/'\n",
    "    valid_image_dir = 'data/DIV2K/ESRGAN/valid/'\n",
    "\n",
    "    image_size = 128\n",
    "    batch_size = 16\n",
    "    num_workers = 4\n",
    "\n",
    "    # Incremental training and migration training\n",
    "    resume = False\n",
    "    strict = False\n",
    "    start_epoch = 0\n",
    "    resume_d_weight = \"\"\n",
    "    resume_g_weight = \"results/RRDBNet_baseline/g-last.pth\"\n",
    "\n",
    "    # Total num epochs\n",
    "    epochs = 48\n",
    "\n",
    "    # Loss function weight\n",
    "    pixel_weight = 1.0\n",
    "    content_weight = 1.0\n",
    "    adversarial_weight = 0.001\n",
    "\n",
    "    # Adam optimizer parameter for Discriminator\n",
    "    d_model_lr = 1e-4\n",
    "    d_model_betas = (0.9, 0.999)\n",
    "\n",
    "    # Adam optimizer parameter for Generator\n",
    "    g_model_lr = 1e-4\n",
    "    g_model_betas = (0.9, 0.999)\n",
    "\n",
    "    # MultiStepLR scheduler parameter for ESRGAN\n",
    "    d_optimizer_milestones = [int(epochs * 0.125), int(epochs * 0.250), int(epochs * 0.500), int(epochs * 0.750)]\n",
    "    g_optimizer_milestones = [int(epochs * 0.125), int(epochs * 0.250), int(epochs * 0.500), int(epochs * 0.750)]\n",
    "    d_optimizer_gamma = 0.5\n",
    "    g_optimizer_gamma = 0.5\n",
    "\n",
    "    # Print the training log every one hundred iterations\n",
    "    print_frequency = 1000\n",
    "\n",
    "if mode == \"valid\":\n",
    "    # Test data address\n",
    "    lr_dir = f\"data/Set14/LRbicx{upscale_factor}\"\n",
    "    sr_dir = f\"results/test/{exp_name}\"\n",
    "    hr_dir = f\"data/Set14/GTmod12\"\n",
    "\n",
    "    model_path = f\"results/{exp_name}/g-last.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ee167",
   "metadata": {
    "papermill": {
     "duration": 0.016564,
     "end_time": "2022-02-17T17:40:17.287701",
     "exception": false,
     "start_time": "2022-02-17T17:40:17.271137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eabbc3",
   "metadata": {
    "papermill": {
     "duration": 0.015763,
     "end_time": "2022-02-17T17:40:17.321809",
     "exception": false,
     "start_time": "2022-02-17T17:40:17.306046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## train_rrdbnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5611cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T17:40:17.396492Z",
     "iopub.status.busy": "2022-02-17T17:40:17.367639Z",
     "iopub.status.idle": "2022-02-17T20:29:34.408975Z",
     "shell.execute_reply": "2022-02-17T20:29:34.408486Z"
    },
    "papermill": {
     "duration": 10157.071355,
     "end_time": "2022-02-17T20:29:34.409154",
     "exception": false,
     "start_time": "2022-02-17T17:40:17.337799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train dataset and valid dataset...\n",
      "Load train dataset and valid dataset successfully.\n",
      "Build RRDBNet model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build RRDBNet model successfully.\n",
      "Define all loss functions...\n",
      "Define all loss functions successfully.\n",
      "Define all optimizer functions...\n",
      "Define all optimizer functions successfully.\n",
      "Define all scheduler functions...\n",
      "Define all scheduler functions successfully.\n",
      "Check whether the training weight is restored...\n",
      "Check whether the training weight is restored successfully.\n",
      "Start train RRDBNet model.\n",
      "Epoch : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: [0/3]\tTime 11.453 (11.453)\tPSNR 16.63 (16.63)\n",
      "* PSNR: 16.59.\n",
      "\n",
      "Epoch : 2\n",
      "Valid: [0/3]\tTime  8.619 ( 8.619)\tPSNR 18.69 (18.69)\n",
      "* PSNR: 18.53.\n",
      "\n",
      "Epoch : 3\n",
      "Valid: [0/3]\tTime  8.616 ( 8.616)\tPSNR 19.95 (19.95)\n",
      "* PSNR: 19.69.\n",
      "\n",
      "Epoch : 4\n",
      "Valid: [0/3]\tTime  8.343 ( 8.343)\tPSNR 20.54 (20.54)\n",
      "* PSNR: 20.27.\n",
      "\n",
      "Epoch : 5\n",
      "Valid: [0/3]\tTime  8.433 ( 8.433)\tPSNR 20.98 (20.98)\n",
      "* PSNR: 20.69.\n",
      "\n",
      "Epoch : 6\n",
      "Valid: [0/3]\tTime  8.306 ( 8.306)\tPSNR 21.16 (21.16)\n",
      "* PSNR: 20.86.\n",
      "\n",
      "Epoch : 7\n",
      "Valid: [0/3]\tTime  8.583 ( 8.583)\tPSNR 21.51 (21.51)\n",
      "* PSNR: 21.19.\n",
      "\n",
      "Epoch : 8\n",
      "Valid: [0/3]\tTime  9.170 ( 9.170)\tPSNR 20.88 (20.88)\n",
      "* PSNR: 20.57.\n",
      "\n",
      "Epoch : 9\n",
      "Valid: [0/3]\tTime  8.673 ( 8.673)\tPSNR 21.82 (21.82)\n",
      "* PSNR: 21.49.\n",
      "\n",
      "Epoch : 10\n",
      "Valid: [0/3]\tTime  8.896 ( 8.896)\tPSNR 22.07 (22.07)\n",
      "* PSNR: 21.74.\n",
      "\n",
      "Epoch : 11\n",
      "Valid: [0/3]\tTime  8.826 ( 8.826)\tPSNR 22.01 (22.01)\n",
      "* PSNR: 21.68.\n",
      "\n",
      "Epoch : 12\n",
      "Valid: [0/3]\tTime  8.570 ( 8.570)\tPSNR 22.45 (22.45)\n",
      "* PSNR: 22.09.\n",
      "\n",
      "Epoch : 13\n",
      "Valid: [0/3]\tTime  8.181 ( 8.181)\tPSNR 21.94 (21.94)\n",
      "* PSNR: 21.61.\n",
      "\n",
      "Epoch : 14\n",
      "Valid: [0/3]\tTime  8.505 ( 8.505)\tPSNR 22.43 (22.43)\n",
      "* PSNR: 22.09.\n",
      "\n",
      "Epoch : 15\n",
      "Valid: [0/3]\tTime  8.622 ( 8.622)\tPSNR 22.44 (22.44)\n",
      "* PSNR: 22.11.\n",
      "\n",
      "Epoch : 16\n",
      "Valid: [0/3]\tTime  8.422 ( 8.422)\tPSNR 22.62 (22.62)\n",
      "* PSNR: 22.26.\n",
      "\n",
      "Epoch : 17\n",
      "Valid: [0/3]\tTime  9.079 ( 9.079)\tPSNR 22.73 (22.73)\n",
      "* PSNR: 22.37.\n",
      "\n",
      "Epoch : 18\n",
      "Valid: [0/3]\tTime  8.332 ( 8.332)\tPSNR 22.90 (22.90)\n",
      "* PSNR: 22.53.\n",
      "\n",
      "Epoch : 19\n",
      "Valid: [0/3]\tTime  9.086 ( 9.086)\tPSNR 22.90 (22.90)\n",
      "* PSNR: 22.53.\n",
      "\n",
      "Epoch : 20\n",
      "Valid: [0/3]\tTime  8.647 ( 8.647)\tPSNR 22.71 (22.71)\n",
      "* PSNR: 22.35.\n",
      "\n",
      "Epoch : 21\n",
      "Valid: [0/3]\tTime  8.811 ( 8.811)\tPSNR 23.00 (23.00)\n",
      "* PSNR: 22.64.\n",
      "\n",
      "Epoch : 22\n",
      "Valid: [0/3]\tTime  8.455 ( 8.455)\tPSNR 22.89 (22.89)\n",
      "* PSNR: 22.53.\n",
      "\n",
      "Epoch : 23\n",
      "Valid: [0/3]\tTime  8.551 ( 8.551)\tPSNR 23.05 (23.05)\n",
      "* PSNR: 22.68.\n",
      "\n",
      "Epoch : 24\n",
      "Valid: [0/3]\tTime  8.217 ( 8.217)\tPSNR 22.25 (22.25)\n",
      "* PSNR: 21.93.\n",
      "\n",
      "Epoch : 25\n",
      "Valid: [0/3]\tTime  8.499 ( 8.499)\tPSNR 23.16 (23.16)\n",
      "* PSNR: 22.79.\n",
      "\n",
      "Epoch : 26\n",
      "Valid: [0/3]\tTime  9.164 ( 9.164)\tPSNR 23.25 (23.25)\n",
      "* PSNR: 22.87.\n",
      "\n",
      "Epoch : 27\n",
      "Valid: [0/3]\tTime  8.379 ( 8.379)\tPSNR 23.29 (23.29)\n",
      "* PSNR: 22.90.\n",
      "\n",
      "Epoch : 28\n",
      "Valid: [0/3]\tTime  9.128 ( 9.128)\tPSNR 23.30 (23.30)\n",
      "* PSNR: 22.91.\n",
      "\n",
      "Epoch : 29\n",
      "Valid: [0/3]\tTime  8.574 ( 8.574)\tPSNR 23.32 (23.32)\n",
      "* PSNR: 22.94.\n",
      "\n",
      "Epoch : 30\n",
      "Valid: [0/3]\tTime  8.364 ( 8.364)\tPSNR 23.35 (23.35)\n",
      "* PSNR: 22.96.\n",
      "\n",
      "Epoch : 31\n",
      "Valid: [0/3]\tTime  8.592 ( 8.592)\tPSNR 23.31 (23.31)\n",
      "* PSNR: 22.92.\n",
      "\n",
      "Epoch : 32\n",
      "Valid: [0/3]\tTime  8.585 ( 8.585)\tPSNR 23.37 (23.37)\n",
      "* PSNR: 22.99.\n",
      "\n",
      "Epoch : 33\n",
      "Valid: [0/3]\tTime  8.307 ( 8.307)\tPSNR 23.40 (23.40)\n",
      "* PSNR: 23.01.\n",
      "\n",
      "Epoch : 34\n",
      "Valid: [0/3]\tTime  8.531 ( 8.531)\tPSNR 23.42 (23.42)\n",
      "* PSNR: 23.03.\n",
      "\n",
      "Epoch : 35\n",
      "Valid: [0/3]\tTime  8.487 ( 8.487)\tPSNR 23.45 (23.45)\n",
      "* PSNR: 23.06.\n",
      "\n",
      "Epoch : 36\n",
      "Valid: [0/3]\tTime  8.226 ( 8.226)\tPSNR 23.40 (23.40)\n",
      "* PSNR: 23.01.\n",
      "\n",
      "Epoch : 37\n",
      "Valid: [0/3]\tTime  9.087 ( 9.087)\tPSNR 23.42 (23.42)\n",
      "* PSNR: 23.03.\n",
      "\n",
      "Epoch : 38\n",
      "Valid: [0/3]\tTime  8.657 ( 8.657)\tPSNR 23.40 (23.40)\n",
      "* PSNR: 23.00.\n",
      "\n",
      "Epoch : 39\n",
      "Valid: [0/3]\tTime  8.435 ( 8.435)\tPSNR 23.49 (23.49)\n",
      "* PSNR: 23.09.\n",
      "\n",
      "Epoch : 40\n",
      "Valid: [0/3]\tTime  8.667 ( 8.667)\tPSNR 23.52 (23.52)\n",
      "* PSNR: 23.13.\n",
      "\n",
      "Epoch : 41\n",
      "Valid: [0/3]\tTime  8.500 ( 8.500)\tPSNR 23.54 (23.54)\n",
      "* PSNR: 23.14.\n",
      "\n",
      "Epoch : 42\n",
      "Valid: [0/3]\tTime  8.959 ( 8.959)\tPSNR 23.56 (23.56)\n",
      "* PSNR: 23.16.\n",
      "\n",
      "Epoch : 43\n",
      "Valid: [0/3]\tTime  8.362 ( 8.362)\tPSNR 23.51 (23.51)\n",
      "* PSNR: 23.12.\n",
      "\n",
      "Epoch : 44\n",
      "Valid: [0/3]\tTime  9.257 ( 9.257)\tPSNR 23.55 (23.55)\n",
      "* PSNR: 23.15.\n",
      "\n",
      "Epoch : 45\n",
      "Valid: [0/3]\tTime  8.738 ( 8.738)\tPSNR 23.49 (23.49)\n",
      "* PSNR: 23.09.\n",
      "\n",
      "Epoch : 46\n",
      "Valid: [0/3]\tTime  9.038 ( 9.038)\tPSNR 23.51 (23.51)\n",
      "* PSNR: 23.11.\n",
      "\n",
      "Epoch : 47\n",
      "Valid: [0/3]\tTime  8.578 ( 8.578)\tPSNR 23.58 (23.58)\n",
      "* PSNR: 23.18.\n",
      "\n",
      "Epoch : 48\n",
      "Valid: [0/3]\tTime  8.897 ( 8.897)\tPSNR 23.53 (23.53)\n",
      "* PSNR: 23.13.\n",
      "\n",
      "Epoch : 49\n",
      "Valid: [0/3]\tTime  8.137 ( 8.137)\tPSNR 23.60 (23.60)\n",
      "* PSNR: 23.20.\n",
      "\n",
      "Epoch : 50\n",
      "Valid: [0/3]\tTime  8.674 ( 8.674)\tPSNR 23.62 (23.62)\n",
      "* PSNR: 23.22.\n",
      "\n",
      "Epoch : 51\n",
      "Valid: [0/3]\tTime  9.228 ( 9.228)\tPSNR 23.62 (23.62)\n",
      "* PSNR: 23.22.\n",
      "\n",
      "Epoch : 52\n",
      "Valid: [0/3]\tTime  8.314 ( 8.314)\tPSNR 23.62 (23.62)\n",
      "* PSNR: 23.22.\n",
      "\n",
      "Epoch : 53\n",
      "Valid: [0/3]\tTime  9.273 ( 9.273)\tPSNR 23.63 (23.63)\n",
      "* PSNR: 23.22.\n",
      "\n",
      "Epoch : 54\n",
      "Valid: [0/3]\tTime  8.626 ( 8.626)\tPSNR 23.64 (23.64)\n",
      "* PSNR: 23.23.\n",
      "\n",
      "Epoch : 55\n",
      "Valid: [0/3]\tTime  8.673 ( 8.673)\tPSNR 23.63 (23.63)\n",
      "* PSNR: 23.23.\n",
      "\n",
      "Epoch : 56\n",
      "Valid: [0/3]\tTime  8.648 ( 8.648)\tPSNR 23.65 (23.65)\n",
      "* PSNR: 23.24.\n",
      "\n",
      "Epoch : 57\n",
      "Valid: [0/3]\tTime  8.530 ( 8.530)\tPSNR 23.66 (23.66)\n",
      "* PSNR: 23.25.\n",
      "\n",
      "Epoch : 58\n",
      "Valid: [0/3]\tTime  9.193 ( 9.193)\tPSNR 23.66 (23.66)\n",
      "* PSNR: 23.25.\n",
      "\n",
      "Epoch : 59\n",
      "Valid: [0/3]\tTime  8.338 ( 8.338)\tPSNR 23.66 (23.66)\n",
      "* PSNR: 23.25.\n",
      "\n",
      "Epoch : 60\n",
      "Valid: [0/3]\tTime  9.321 ( 9.321)\tPSNR 23.65 (23.65)\n",
      "* PSNR: 23.24.\n",
      "\n",
      "Epoch : 61\n",
      "Valid: [0/3]\tTime  8.645 ( 8.645)\tPSNR 23.67 (23.67)\n",
      "* PSNR: 23.26.\n",
      "\n",
      "Epoch : 62\n",
      "Valid: [0/3]\tTime  8.143 ( 8.143)\tPSNR 23.66 (23.66)\n",
      "* PSNR: 23.25.\n",
      "\n",
      "Epoch : 63\n",
      "Valid: [0/3]\tTime  8.721 ( 8.721)\tPSNR 23.66 (23.66)\n",
      "* PSNR: 23.26.\n",
      "\n",
      "Epoch : 64\n",
      "Valid: [0/3]\tTime  8.468 ( 8.468)\tPSNR 23.68 (23.68)\n",
      "* PSNR: 23.27.\n",
      "\n",
      "Epoch : 65\n",
      "Valid: [0/3]\tTime  8.671 ( 8.671)\tPSNR 23.68 (23.68)\n",
      "* PSNR: 23.27.\n",
      "\n",
      "Epoch : 66\n",
      "Valid: [0/3]\tTime  8.631 ( 8.631)\tPSNR 23.68 (23.68)\n",
      "* PSNR: 23.27.\n",
      "\n",
      "Epoch : 67\n",
      "Valid: [0/3]\tTime  9.214 ( 9.214)\tPSNR 23.68 (23.68)\n",
      "* PSNR: 23.27.\n",
      "\n",
      "Epoch : 68\n",
      "Valid: [0/3]\tTime  8.277 ( 8.277)\tPSNR 23.70 (23.70)\n",
      "* PSNR: 23.29.\n",
      "\n",
      "Epoch : 69\n",
      "Valid: [0/3]\tTime  8.524 ( 8.524)\tPSNR 23.70 (23.70)\n",
      "* PSNR: 23.29.\n",
      "\n",
      "Epoch : 70\n",
      "Valid: [0/3]\tTime  8.653 ( 8.653)\tPSNR 23.69 (23.69)\n",
      "* PSNR: 23.28.\n",
      "\n",
      "Epoch : 71\n",
      "Valid: [0/3]\tTime  8.268 ( 8.268)\tPSNR 23.71 (23.71)\n",
      "* PSNR: 23.29.\n",
      "\n",
      "Epoch : 72\n",
      "Valid: [0/3]\tTime  9.160 ( 9.160)\tPSNR 23.69 (23.69)\n",
      "* PSNR: 23.28.\n",
      "\n",
      "Epoch : 73\n",
      "Valid: [0/3]\tTime  8.557 ( 8.557)\tPSNR 23.72 (23.72)\n",
      "* PSNR: 23.30.\n",
      "\n",
      "Epoch : 74\n",
      "Valid: [0/3]\tTime  9.133 ( 9.133)\tPSNR 23.72 (23.72)\n",
      "* PSNR: 23.30.\n",
      "\n",
      "Epoch : 75\n",
      "Valid: [0/3]\tTime  8.599 ( 8.599)\tPSNR 23.72 (23.72)\n",
      "* PSNR: 23.31.\n",
      "\n",
      "Epoch : 76\n",
      "Valid: [0/3]\tTime  8.607 ( 8.607)\tPSNR 23.72 (23.72)\n",
      "* PSNR: 23.31.\n",
      "\n",
      "Epoch : 77\n",
      "Valid: [0/3]\tTime  8.353 ( 8.353)\tPSNR 23.73 (23.73)\n",
      "* PSNR: 23.31.\n",
      "\n",
      "Epoch : 78\n",
      "Valid: [0/3]\tTime  8.670 ( 8.670)\tPSNR 23.73 (23.73)\n",
      "* PSNR: 23.31.\n",
      "\n",
      "Epoch : 79\n",
      "Valid: [0/3]\tTime  9.359 ( 9.359)\tPSNR 23.73 (23.73)\n",
      "* PSNR: 23.31.\n",
      "\n",
      "Epoch : 80\n",
      "Valid: [0/3]\tTime  8.507 ( 8.507)\tPSNR 23.73 (23.73)\n",
      "* PSNR: 23.31.\n",
      "\n",
      "Epoch : 81\n",
      "Valid: [0/3]\tTime  9.042 ( 9.042)\tPSNR 23.73 (23.73)\n",
      "* PSNR: 23.32.\n",
      "\n",
      "Epoch : 82\n",
      "Valid: [0/3]\tTime  8.515 ( 8.515)\tPSNR 23.74 (23.74)\n",
      "* PSNR: 23.32.\n",
      "\n",
      "Epoch : 83\n",
      "Valid: [0/3]\tTime  8.652 ( 8.652)\tPSNR 23.74 (23.74)\n",
      "* PSNR: 23.32.\n",
      "\n",
      "Epoch : 84\n",
      "Valid: [0/3]\tTime  8.181 ( 8.181)\tPSNR 23.74 (23.74)\n",
      "* PSNR: 23.32.\n",
      "\n",
      "Epoch : 85\n",
      "Valid: [0/3]\tTime  8.496 ( 8.496)\tPSNR 23.74 (23.74)\n",
      "* PSNR: 23.32.\n",
      "\n",
      "Epoch : 86\n",
      "Valid: [0/3]\tTime  9.134 ( 9.134)\tPSNR 23.74 (23.74)\n",
      "* PSNR: 23.33.\n",
      "\n",
      "Epoch : 87\n",
      "Valid: [0/3]\tTime  8.375 ( 8.375)\tPSNR 23.74 (23.74)\n",
      "* PSNR: 23.33.\n",
      "\n",
      "Epoch : 88\n",
      "Valid: [0/3]\tTime  9.339 ( 9.339)\tPSNR 23.75 (23.75)\n",
      "* PSNR: 23.33.\n",
      "\n",
      "Epoch : 89\n",
      "Valid: [0/3]\tTime  8.703 ( 8.703)\tPSNR 23.75 (23.75)\n",
      "* PSNR: 23.33.\n",
      "\n",
      "Epoch : 90\n",
      "Valid: [0/3]\tTime  8.244 ( 8.244)\tPSNR 23.75 (23.75)\n",
      "* PSNR: 23.33.\n",
      "\n",
      "Epoch : 91\n",
      "Valid: [0/3]\tTime  8.512 ( 8.512)\tPSNR 23.75 (23.75)\n",
      "* PSNR: 23.33.\n",
      "\n",
      "Epoch : 92\n",
      "Valid: [0/3]\tTime  8.713 ( 8.713)\tPSNR 23.76 (23.76)\n",
      "* PSNR: 23.34.\n",
      "\n",
      "Epoch : 93\n",
      "Valid: [0/3]\tTime  8.202 ( 8.202)\tPSNR 23.76 (23.76)\n",
      "* PSNR: 23.34.\n",
      "\n",
      "Epoch : 94\n",
      "Valid: [0/3]\tTime  8.495 ( 8.495)\tPSNR 23.75 (23.75)\n",
      "* PSNR: 23.33.\n",
      "\n",
      "Epoch : 95\n",
      "Valid: [0/3]\tTime  9.305 ( 9.305)\tPSNR 23.76 (23.76)\n",
      "* PSNR: 23.34.\n",
      "\n",
      "Epoch : 96\n",
      "Valid: [0/3]\tTime  8.234 ( 8.234)\tPSNR 23.76 (23.76)\n",
      "* PSNR: 23.34.\n",
      "\n",
      "Epoch : 97\n",
      "Valid: [0/3]\tTime  9.282 ( 9.282)\tPSNR 23.76 (23.76)\n",
      "* PSNR: 23.34.\n",
      "\n",
      "Epoch : 98\n",
      "Valid: [0/3]\tTime  8.589 ( 8.589)\tPSNR 23.77 (23.77)\n",
      "* PSNR: 23.35.\n",
      "\n",
      "Epoch : 99\n",
      "Valid: [0/3]\tTime  8.637 ( 8.637)\tPSNR 23.77 (23.77)\n",
      "* PSNR: 23.35.\n",
      "\n",
      "Epoch : 100\n",
      "Valid: [0/3]\tTime  8.853 ( 8.853)\tPSNR 23.77 (23.77)\n",
      "* PSNR: 23.35.\n",
      "\n",
      "Epoch : 101\n",
      "Valid: [0/3]\tTime  8.595 ( 8.595)\tPSNR 23.77 (23.77)\n",
      "* PSNR: 23.35.\n",
      "\n",
      "Epoch : 102\n",
      "Valid: [0/3]\tTime  9.170 ( 9.170)\tPSNR 23.77 (23.77)\n",
      "* PSNR: 23.35.\n",
      "\n",
      "Epoch : 103\n",
      "Valid: [0/3]\tTime  8.450 ( 8.450)\tPSNR 23.77 (23.77)\n",
      "* PSNR: 23.35.\n",
      "\n",
      "Epoch : 104\n",
      "Valid: [0/3]\tTime  9.122 ( 9.122)\tPSNR 23.77 (23.77)\n",
      "* PSNR: 23.35.\n",
      "\n",
      "Epoch : 105\n",
      "Valid: [0/3]\tTime  8.626 ( 8.626)\tPSNR 23.77 (23.77)\n",
      "* PSNR: 23.35.\n",
      "\n",
      "Epoch : 106\n",
      "Valid: [0/3]\tTime  8.294 ( 8.294)\tPSNR 23.78 (23.78)\n",
      "* PSNR: 23.35.\n",
      "\n",
      "Epoch : 107\n",
      "Valid: [0/3]\tTime  8.462 ( 8.462)\tPSNR 23.78 (23.78)\n",
      "* PSNR: 23.35.\n",
      "\n",
      "Epoch : 108\n",
      "Valid: [0/3]\tTime  8.642 ( 8.642)\tPSNR 23.78 (23.78)\n",
      "* PSNR: 23.36.\n",
      "\n",
      "Epoch : 109\n",
      "Valid: [0/3]\tTime  8.769 ( 8.769)\tPSNR 23.78 (23.78)\n",
      "* PSNR: 23.36.\n",
      "\n",
      "Epoch : 110\n",
      "Valid: [0/3]\tTime  8.602 ( 8.602)\tPSNR 23.78 (23.78)\n",
      "* PSNR: 23.36.\n",
      "\n",
      "Epoch : 111\n",
      "Valid: [0/3]\tTime  9.237 ( 9.237)\tPSNR 23.78 (23.78)\n",
      "* PSNR: 23.36.\n",
      "\n",
      "Epoch : 112\n",
      "Valid: [0/3]\tTime  8.170 ( 8.170)\tPSNR 23.78 (23.78)\n",
      "* PSNR: 23.36.\n",
      "\n",
      "Epoch : 113\n",
      "Valid: [0/3]\tTime  8.528 ( 8.528)\tPSNR 23.78 (23.78)\n",
      "* PSNR: 23.36.\n",
      "\n",
      "Epoch : 114\n",
      "Valid: [0/3]\tTime  8.630 ( 8.630)\tPSNR 23.78 (23.78)\n",
      "* PSNR: 23.36.\n",
      "\n",
      "Epoch : 115\n",
      "Valid: [0/3]\tTime  8.282 ( 8.282)\tPSNR 23.78 (23.78)\n",
      "* PSNR: 23.36.\n",
      "\n",
      "Epoch : 116\n",
      "Valid: [0/3]\tTime  8.632 ( 8.632)\tPSNR 23.79 (23.79)\n",
      "* PSNR: 23.36.\n",
      "\n",
      "Epoch : 117\n",
      "Valid: [0/3]\tTime  8.566 ( 8.566)\tPSNR 23.79 (23.79)\n",
      "* PSNR: 23.36.\n",
      "\n",
      "Epoch : 118\n",
      "Valid: [0/3]\tTime  8.935 ( 8.935)\tPSNR 23.79 (23.79)\n",
      "* PSNR: 23.37.\n",
      "\n",
      "Epoch : 119\n",
      "Valid: [0/3]\tTime  8.574 ( 8.574)\tPSNR 23.79 (23.79)\n",
      "* PSNR: 23.37.\n",
      "\n",
      "Epoch : 120\n",
      "Valid: [0/3]\tTime  9.196 ( 9.196)\tPSNR 23.79 (23.79)\n",
      "* PSNR: 23.37.\n",
      "\n",
      "End train RRDBNet model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.cuda import amp\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def main():\n",
    "    print(\"Load train dataset and valid dataset...\")\n",
    "    train_dataloader, valid_dataloader = load_dataset()\n",
    "    print(\"Load train dataset and valid dataset successfully.\")\n",
    "\n",
    "    print(\"Build RRDBNet model...\")\n",
    "    model = build_model()\n",
    "    print(\"Build RRDBNet model successfully.\")\n",
    "\n",
    "    print(\"Define all loss functions...\")\n",
    "    psnr_criterion, pixel_criterion = define_loss()\n",
    "    print(\"Define all loss functions successfully.\")\n",
    "\n",
    "    print(\"Define all optimizer functions...\")\n",
    "    optimizer = define_optimizer(model)\n",
    "    print(\"Define all optimizer functions successfully.\")\n",
    "\n",
    "    print(\"Define all scheduler functions...\")\n",
    "    scheduler = define_scheduler(optimizer)\n",
    "    print(\"Define all scheduler functions successfully.\")\n",
    "\n",
    "    print(\"Check whether the training weight is restored...\")\n",
    "    resume_checkpoint(model)\n",
    "    print(\"Check whether the training weight is restored successfully.\")\n",
    "\n",
    "    # Create a folder of super-resolution experiment results\n",
    "    samples_dir = os.path.join(\"samples\", exp_name)\n",
    "    results_dir = os.path.join(\"results\", exp_name)\n",
    "    if not os.path.exists(samples_dir):\n",
    "        os.makedirs(samples_dir)\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    # Create training process log file\n",
    "    writer = SummaryWriter(os.path.join(\"samples\", \"logs\", exp_name))\n",
    "\n",
    "    # Initialize the gradient scaler\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    # Initialize training to generate network evaluation indicators\n",
    "    best_psnr = 0.0\n",
    "\n",
    "    print(\"Start train RRDBNet model.\")\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"Epoch : {epoch+1}\")\n",
    "        train(model, train_dataloader, psnr_criterion, pixel_criterion, optimizer, epoch, scaler, writer)\n",
    "\n",
    "        psnr = validate(model, valid_dataloader, psnr_criterion, epoch, writer)\n",
    "        # Automatically save the model with the highest index\n",
    "        is_best = psnr > best_psnr\n",
    "        best_psnr = max(psnr, best_psnr)\n",
    "        torch.save(model.state_dict(), os.path.join(samples_dir, f\"g_epoch_{epoch + 1}.pth\"))\n",
    "        if is_best:\n",
    "            torch.save(model.state_dict(), os.path.join(results_dir, \"g-best.pth\"))\n",
    "\n",
    "        # Update LR\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save the generator weight under the last Epoch in this stage\n",
    "    torch.save(model.state_dict(), os.path.join(results_dir, \"g-last.pth\"))\n",
    "    print(\"End train RRDBNet model.\")\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    train_datasets = ImageDataset(train_image_dir, image_size, upscale_factor, \"train\")\n",
    "    valid_datasets = ImageDataset(valid_image_dir, image_size, upscale_factor, \"valid\")\n",
    "    # Make it into a data set type supported by PyTorch\n",
    "    train_dataloader = DataLoader(train_datasets,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  persistent_workers=True)\n",
    "    valid_dataloader = DataLoader(valid_datasets,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  persistent_workers=True)\n",
    "\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "\n",
    "def build_model() -> nn.Module:\n",
    "    model = Generator().to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def define_loss():\n",
    "    psnr_criterion = nn.MSELoss().to(device)\n",
    "    pixel_criterion = nn.L1Loss().to(device)\n",
    "\n",
    "    return psnr_criterion, pixel_criterion\n",
    "\n",
    "\n",
    "def define_optimizer(model) -> optim.Adam:\n",
    "    optimizer = optim.Adam(model.parameters(), model_lr, model_betas)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def define_scheduler(optimizer) -> optim.lr_scheduler:\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def resume_checkpoint(model) -> None:\n",
    "    if resume:\n",
    "        if resume_weight != \"\":\n",
    "            # Get pretrained model state dict\n",
    "            pretrained_state_dict = torch.load(resume_weight)\n",
    "            model_state_dict = model.state_dict()\n",
    "            # Extract the fitted model weights\n",
    "            new_state_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict.items()}\n",
    "            # Overwrite the pretrained model weights to the current model\n",
    "            model_state_dict.update(new_state_dict)\n",
    "            model.load_state_dict(model_state_dict, strict=strict)\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, psnr_criterion, pixel_criterion, optimizer, epoch, scaler, writer) -> None:\n",
    "    # Calculate how many iterations there are under epoch\n",
    "    batches = len(train_dataloader)\n",
    "\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    data_time = AverageMeter(\"Data\", \":6.3f\")\n",
    "    losses = AverageMeter(\"Loss\", \":6.6f\")\n",
    "    psnres = AverageMeter(\"PSNR\", \":4.2f\")\n",
    "    progress = ProgressMeter(batches, [batch_time, data_time, losses, psnres], prefix=f\"Epoch: [{epoch + 1}]\")\n",
    "\n",
    "    # Put the generator in train mode.\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for index, (lr, hr) in enumerate(train_dataloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        lr = lr.to(device, non_blocking=True)\n",
    "        hr = hr.to(device, non_blocking=True)\n",
    "\n",
    "        # Initialize the generator gradient\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with amp.autocast():\n",
    "            sr = model(lr)\n",
    "            loss = pixel_criterion(sr, hr)\n",
    "\n",
    "        # Gradient zoom\n",
    "        scaler.scale(loss).backward()\n",
    "        # Update generator weight\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        psnr = 10. * torch.log10(1. / psnr_criterion(sr, hr))\n",
    "        losses.update(loss.item(), lr.size(0))\n",
    "        psnres.update(psnr.item(), lr.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # Writer Loss to file\n",
    "        writer.add_scalar(\"Train/Loss\", loss.item(), index + epoch * batches + 1)\n",
    "        if index % print_frequency == 0 and index != 0:\n",
    "            progress.display(index)\n",
    "\n",
    "\n",
    "def validate(model, valid_dataloader, psnr_criterion, epoch, writer) -> float:\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    psnres = AverageMeter(\"PSNR\", \":4.2f\")\n",
    "    progress = ProgressMeter(len(valid_dataloader), [batch_time, psnres], prefix=\"Valid: \")\n",
    "\n",
    "    # Put the generator in verification mode.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for index, (lr, hr) in enumerate(valid_dataloader):\n",
    "            lr = lr.to(device, non_blocking=True)\n",
    "            hr = hr.to(device, non_blocking=True)\n",
    "\n",
    "            # Mixed precision\n",
    "            with amp.autocast():\n",
    "                sr = model(lr)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            psnr = 10. * torch.log10(1. / psnr_criterion(sr, hr))\n",
    "            psnres.update(psnr.item(), hr.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if index % print_frequency == 0:\n",
    "                progress.display(index)\n",
    "\n",
    "        writer.add_scalar(\"Valid/PSNR\", psnres.avg, epoch + 1)\n",
    "        # Print evaluation indicators.\n",
    "        print(f\"* PSNR: {psnres.avg:4.2f}.\\n\")\n",
    "\n",
    "    return psnres.avg\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=\":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print(\"\\t\".join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
    "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418d303",
   "metadata": {
    "papermill": {
     "duration": 0.128825,
     "end_time": "2022-02-17T20:29:34.665716",
     "exception": false,
     "start_time": "2022-02-17T20:29:34.536891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a04dd1",
   "metadata": {
    "papermill": {
     "duration": 0.126661,
     "end_time": "2022-02-17T20:29:34.920933",
     "exception": false,
     "start_time": "2022-02-17T20:29:34.794272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6510873f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T20:29:35.203208Z",
     "iopub.status.busy": "2022-02-17T20:29:35.201604Z",
     "iopub.status.idle": "2022-02-17T20:29:35.203935Z",
     "shell.execute_reply": "2022-02-17T20:29:35.204403Z"
    },
    "papermill": {
     "duration": 0.155065,
     "end_time": "2022-02-17T20:29:35.204569",
     "exception": false,
     "start_time": "2022-02-17T20:29:35.049504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.backends import cudnn\n",
    "# Random seed to maintain reproducible results\n",
    "torch.manual_seed(0)\n",
    "# Use GPU for training by default\n",
    "if torch.cuda.is_available():\n",
    "    device =  torch.device('cuda')\n",
    "else:\n",
    "    device =  torch.device('cpu')\n",
    "# device = torch.device(\"cuda\", 0)\n",
    "# Turning on when the image size does not change during training can speed up training\n",
    "cudnn.benchmark = True\n",
    "# Image magnification factor\n",
    "upscale_factor = 4\n",
    "# Current configuration parameter method\n",
    "mode = \"train_esrgan\"\n",
    "# Experiment name, easy to save weights and log files\n",
    "exp_name = \"RRDBNet_baseline\"\n",
    "\n",
    "if mode == \"train_rrdbnet\":\n",
    "    # Dataset address\n",
    "    train_image_dir = 'data/DIV2K/ESRGAN/train/'\n",
    "    valid_image_dir = 'data/DIV2K/ESRGAN/valid/'\n",
    "\n",
    "    image_size = 192\n",
    "    batch_size = 48\n",
    "    num_workers = 4\n",
    "\n",
    "    # Incremental training and migration training\n",
    "    resume = False\n",
    "    strict = False\n",
    "    start_epoch = 0\n",
    "    resume_weight = \"\"\n",
    "\n",
    "    # Total num epochs\n",
    "    epochs = 120\n",
    "\n",
    "    # Adam optimizer parameter for RRDBNet(p)\n",
    "    model_lr = 2e-4\n",
    "    model_betas = (0.9, 0.999)\n",
    "\n",
    "    # StepLR scheduler\n",
    "    step_size = epochs // 5\n",
    "    gamma = 0.5\n",
    "\n",
    "    # Print the training log every one hundred iterations\n",
    "    print_frequency = 1000\n",
    "\n",
    "if mode == \"train_esrgan\":\n",
    "    # Dataset address\n",
    "    train_image_dir = 'data/DIV2K/ESRGAN/train/'\n",
    "    valid_image_dir = 'data/DIV2K/ESRGAN/valid/'\n",
    "\n",
    "    image_size = 128\n",
    "    batch_size = 32\n",
    "    num_workers = 4\n",
    "\n",
    "    # Incremental training and migration training\n",
    "    resume = False\n",
    "    strict = False\n",
    "    start_epoch = 0\n",
    "    resume_d_weight = \"\"\n",
    "    resume_g_weight = \"results/RRDBNet_baseline/g-last.pth\"\n",
    "\n",
    "    # Total num epochs\n",
    "    epochs = 120\n",
    "\n",
    "    # Loss function weight\n",
    "    pixel_weight = 1.0\n",
    "    content_weight = 1.0\n",
    "    adversarial_weight = 0.001\n",
    "\n",
    "    # Adam optimizer parameter for Discriminator\n",
    "    d_model_lr = 1e-4\n",
    "    d_model_betas = (0.9, 0.999)\n",
    "\n",
    "    # Adam optimizer parameter for Generator\n",
    "    g_model_lr = 1e-4\n",
    "    g_model_betas = (0.9, 0.999)\n",
    "\n",
    "    # MultiStepLR scheduler parameter for ESRGAN\n",
    "    d_optimizer_milestones = [int(epochs * 0.125), int(epochs * 0.250), int(epochs * 0.500), int(epochs * 0.750)]\n",
    "    g_optimizer_milestones = [int(epochs * 0.125), int(epochs * 0.250), int(epochs * 0.500), int(epochs * 0.750)]\n",
    "    d_optimizer_gamma = 0.5\n",
    "    g_optimizer_gamma = 0.5\n",
    "\n",
    "    # Print the training log every one hundred iterations\n",
    "    print_frequency = 1000\n",
    "\n",
    "if mode == \"valid\":\n",
    "    # Test data address\n",
    "    lr_dir = f\"data/Set14/LRbicx{upscale_factor}\"\n",
    "    sr_dir = f\"results/test/{exp_name}\"\n",
    "    hr_dir = f\"data/Set14/GTmod12\"\n",
    "\n",
    "    model_path = f\"results/{exp_name}/g-last.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e2087",
   "metadata": {
    "papermill": {
     "duration": 0.1258,
     "end_time": "2022-02-17T20:29:35.468778",
     "exception": false,
     "start_time": "2022-02-17T20:29:35.342978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## train_esrgan.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22592c0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T20:29:35.777440Z",
     "iopub.status.busy": "2022-02-17T20:29:35.760221Z",
     "iopub.status.idle": "2022-02-17T23:12:18.182367Z",
     "shell.execute_reply": "2022-02-17T23:12:18.182844Z"
    },
    "papermill": {
     "duration": 9762.589172,
     "end_time": "2022-02-17T23:12:18.183071",
     "exception": false,
     "start_time": "2022-02-17T20:29:35.593899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train dataset and valid dataset...\n",
      "Load train dataset and valid dataset successfully.\n",
      "Build ESRGAN model...\n",
      "Build ESRGAN model successfully.\n",
      "Define all loss functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6471ab88108472780ccc16f2f9bf414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define all loss functions successfully.\n",
      "Define all optimizer functions...\n",
      "Define all optimizer functions successfully.\n",
      "Define all optimizer scheduler...\n",
      "Define all optimizer scheduler functions successfully.\n",
      "Check whether the training weight is restored...\n",
      "Check whether the training weight is restored successfully.\n",
      "Start train ESRGAN model.\n",
      "Valid: [0/4]\tTime  9.603 ( 9.603)\tPSNR 16.52 (16.52)\n",
      "* PSNR: 16.22.\n",
      "\n",
      "Valid: [0/4]\tTime  8.082 ( 8.082)\tPSNR 17.14 (17.14)\n",
      "* PSNR: 16.68.\n",
      "\n",
      "Valid: [0/4]\tTime  8.529 ( 8.529)\tPSNR 17.70 (17.70)\n",
      "* PSNR: 17.22.\n",
      "\n",
      "Valid: [0/4]\tTime  8.114 ( 8.114)\tPSNR 17.78 (17.78)\n",
      "* PSNR: 17.35.\n",
      "\n",
      "Valid: [0/4]\tTime  8.325 ( 8.325)\tPSNR 19.50 (19.50)\n",
      "* PSNR: 18.99.\n",
      "\n",
      "Valid: [0/4]\tTime  7.874 ( 7.874)\tPSNR 20.10 (20.10)\n",
      "* PSNR: 19.57.\n",
      "\n",
      "Valid: [0/4]\tTime  7.825 ( 7.825)\tPSNR 20.75 (20.75)\n",
      "* PSNR: 20.17.\n",
      "\n",
      "Valid: [0/4]\tTime  8.101 ( 8.101)\tPSNR 19.47 (19.47)\n",
      "* PSNR: 18.94.\n",
      "\n",
      "Valid: [0/4]\tTime  8.144 ( 8.144)\tPSNR 21.12 (21.12)\n",
      "* PSNR: 20.50.\n",
      "\n",
      "Valid: [0/4]\tTime  7.829 ( 7.829)\tPSNR 21.43 (21.43)\n",
      "* PSNR: 20.80.\n",
      "\n",
      "Valid: [0/4]\tTime  8.247 ( 8.247)\tPSNR 20.51 (20.51)\n",
      "* PSNR: 19.89.\n",
      "\n",
      "Valid: [0/4]\tTime  8.032 ( 8.032)\tPSNR 20.98 (20.98)\n",
      "* PSNR: 20.36.\n",
      "\n",
      "Valid: [0/4]\tTime  7.677 ( 7.677)\tPSNR 21.67 (21.67)\n",
      "* PSNR: 21.03.\n",
      "\n",
      "Valid: [0/4]\tTime  7.813 ( 7.813)\tPSNR 21.58 (21.58)\n",
      "* PSNR: 20.94.\n",
      "\n",
      "Valid: [0/4]\tTime  7.828 ( 7.828)\tPSNR 21.73 (21.73)\n",
      "* PSNR: 21.05.\n",
      "\n",
      "Valid: [0/4]\tTime  8.085 ( 8.085)\tPSNR 21.78 (21.78)\n",
      "* PSNR: 21.09.\n",
      "\n",
      "Valid: [0/4]\tTime  8.167 ( 8.167)\tPSNR 21.91 (21.91)\n",
      "* PSNR: 21.21.\n",
      "\n",
      "Valid: [0/4]\tTime  8.293 ( 8.293)\tPSNR 21.96 (21.96)\n",
      "* PSNR: 21.27.\n",
      "\n",
      "Valid: [0/4]\tTime  8.053 ( 8.053)\tPSNR 21.76 (21.76)\n",
      "* PSNR: 21.08.\n",
      "\n",
      "Valid: [0/4]\tTime  8.025 ( 8.025)\tPSNR 22.11 (22.11)\n",
      "* PSNR: 21.40.\n",
      "\n",
      "Valid: [0/4]\tTime  7.810 ( 7.810)\tPSNR 22.04 (22.04)\n",
      "* PSNR: 21.33.\n",
      "\n",
      "Valid: [0/4]\tTime  8.664 ( 8.664)\tPSNR 21.77 (21.77)\n",
      "* PSNR: 21.07.\n",
      "\n",
      "Valid: [0/4]\tTime  7.859 ( 7.859)\tPSNR 21.84 (21.84)\n",
      "* PSNR: 21.14.\n",
      "\n",
      "Valid: [0/4]\tTime  8.429 ( 8.429)\tPSNR 21.91 (21.91)\n",
      "* PSNR: 21.19.\n",
      "\n",
      "Valid: [0/4]\tTime  8.137 ( 8.137)\tPSNR 22.04 (22.04)\n",
      "* PSNR: 21.33.\n",
      "\n",
      "Valid: [0/4]\tTime  7.208 ( 7.208)\tPSNR 21.99 (21.99)\n",
      "* PSNR: 21.27.\n",
      "\n",
      "Valid: [0/4]\tTime  8.151 ( 8.151)\tPSNR 22.01 (22.01)\n",
      "* PSNR: 21.30.\n",
      "\n",
      "Valid: [0/4]\tTime  7.746 ( 7.746)\tPSNR 22.00 (22.00)\n",
      "* PSNR: 21.30.\n",
      "\n",
      "Valid: [0/4]\tTime  7.973 ( 7.973)\tPSNR 21.89 (21.89)\n",
      "* PSNR: 21.19.\n",
      "\n",
      "Valid: [0/4]\tTime  8.250 ( 8.250)\tPSNR 22.04 (22.04)\n",
      "* PSNR: 21.32.\n",
      "\n",
      "Valid: [0/4]\tTime  7.732 ( 7.732)\tPSNR 22.03 (22.03)\n",
      "* PSNR: 21.30.\n",
      "\n",
      "Valid: [0/4]\tTime  8.206 ( 8.206)\tPSNR 22.19 (22.19)\n",
      "* PSNR: 21.46.\n",
      "\n",
      "Valid: [0/4]\tTime  8.768 ( 8.768)\tPSNR 22.05 (22.05)\n",
      "* PSNR: 21.32.\n",
      "\n",
      "Valid: [0/4]\tTime  8.074 ( 8.074)\tPSNR 22.03 (22.03)\n",
      "* PSNR: 21.30.\n",
      "\n",
      "Valid: [0/4]\tTime  8.497 ( 8.497)\tPSNR 22.18 (22.18)\n",
      "* PSNR: 21.45.\n",
      "\n",
      "Valid: [0/4]\tTime  7.785 ( 7.785)\tPSNR 22.08 (22.08)\n",
      "* PSNR: 21.35.\n",
      "\n",
      "Valid: [0/4]\tTime  8.627 ( 8.627)\tPSNR 21.98 (21.98)\n",
      "* PSNR: 21.26.\n",
      "\n",
      "Valid: [0/4]\tTime  8.058 ( 8.058)\tPSNR 22.08 (22.08)\n",
      "* PSNR: 21.34.\n",
      "\n",
      "Valid: [0/4]\tTime  8.357 ( 8.357)\tPSNR 22.22 (22.22)\n",
      "* PSNR: 21.48.\n",
      "\n",
      "Valid: [0/4]\tTime  8.343 ( 8.343)\tPSNR 22.19 (22.19)\n",
      "* PSNR: 21.46.\n",
      "\n",
      "Valid: [0/4]\tTime  7.730 ( 7.730)\tPSNR 22.00 (22.00)\n",
      "* PSNR: 21.28.\n",
      "\n",
      "Valid: [0/4]\tTime  8.141 ( 8.141)\tPSNR 22.11 (22.11)\n",
      "* PSNR: 21.38.\n",
      "\n",
      "Valid: [0/4]\tTime  8.197 ( 8.197)\tPSNR 22.14 (22.14)\n",
      "* PSNR: 21.40.\n",
      "\n",
      "Valid: [0/4]\tTime  8.076 ( 8.076)\tPSNR 22.21 (22.21)\n",
      "* PSNR: 21.47.\n",
      "\n",
      "Valid: [0/4]\tTime  8.066 ( 8.066)\tPSNR 22.26 (22.26)\n",
      "* PSNR: 21.52.\n",
      "\n",
      "Valid: [0/4]\tTime  7.951 ( 7.951)\tPSNR 22.20 (22.20)\n",
      "* PSNR: 21.46.\n",
      "\n",
      "Valid: [0/4]\tTime  8.396 ( 8.396)\tPSNR 22.27 (22.27)\n",
      "* PSNR: 21.52.\n",
      "\n",
      "Valid: [0/4]\tTime  7.723 ( 7.723)\tPSNR 22.12 (22.12)\n",
      "* PSNR: 21.39.\n",
      "\n",
      "Valid: [0/4]\tTime  8.149 ( 8.149)\tPSNR 22.08 (22.08)\n",
      "* PSNR: 21.34.\n",
      "\n",
      "Valid: [0/4]\tTime  7.798 ( 7.798)\tPSNR 22.10 (22.10)\n",
      "* PSNR: 21.38.\n",
      "\n",
      "Valid: [0/4]\tTime  8.168 ( 8.168)\tPSNR 22.17 (22.17)\n",
      "* PSNR: 21.44.\n",
      "\n",
      "Valid: [0/4]\tTime  8.359 ( 8.359)\tPSNR 22.05 (22.05)\n",
      "* PSNR: 21.32.\n",
      "\n",
      "Valid: [0/4]\tTime  7.416 ( 7.416)\tPSNR 22.22 (22.22)\n",
      "* PSNR: 21.49.\n",
      "\n",
      "Valid: [0/4]\tTime  8.899 ( 8.899)\tPSNR 22.16 (22.16)\n",
      "* PSNR: 21.42.\n",
      "\n",
      "Valid: [0/4]\tTime  7.916 ( 7.916)\tPSNR 22.27 (22.27)\n",
      "* PSNR: 21.52.\n",
      "\n",
      "Valid: [0/4]\tTime  8.752 ( 8.752)\tPSNR 22.18 (22.18)\n",
      "* PSNR: 21.45.\n",
      "\n",
      "Valid: [0/4]\tTime  7.994 ( 7.994)\tPSNR 22.15 (22.15)\n",
      "* PSNR: 21.42.\n",
      "\n",
      "Valid: [0/4]\tTime  8.448 ( 8.448)\tPSNR 22.24 (22.24)\n",
      "* PSNR: 21.51.\n",
      "\n",
      "Valid: [0/4]\tTime  8.224 ( 8.224)\tPSNR 22.20 (22.20)\n",
      "* PSNR: 21.47.\n",
      "\n",
      "Valid: [0/4]\tTime  8.536 ( 8.536)\tPSNR 22.18 (22.18)\n",
      "* PSNR: 21.43.\n",
      "\n",
      "Valid: [0/4]\tTime  8.192 ( 8.192)\tPSNR 22.24 (22.24)\n",
      "* PSNR: 21.49.\n",
      "\n",
      "Valid: [0/4]\tTime  8.719 ( 8.719)\tPSNR 22.20 (22.20)\n",
      "* PSNR: 21.45.\n",
      "\n",
      "Valid: [0/4]\tTime  7.613 ( 7.613)\tPSNR 22.23 (22.23)\n",
      "* PSNR: 21.48.\n",
      "\n",
      "Valid: [0/4]\tTime  8.684 ( 8.684)\tPSNR 22.25 (22.25)\n",
      "* PSNR: 21.52.\n",
      "\n",
      "Valid: [0/4]\tTime  7.876 ( 7.876)\tPSNR 22.21 (22.21)\n",
      "* PSNR: 21.47.\n",
      "\n",
      "Valid: [0/4]\tTime  8.932 ( 8.932)\tPSNR 22.23 (22.23)\n",
      "* PSNR: 21.49.\n",
      "\n",
      "Valid: [0/4]\tTime  8.190 ( 8.190)\tPSNR 22.24 (22.24)\n",
      "* PSNR: 21.50.\n",
      "\n",
      "Valid: [0/4]\tTime  9.750 ( 9.750)\tPSNR 22.17 (22.17)\n",
      "* PSNR: 21.43.\n",
      "\n",
      "Valid: [0/4]\tTime  8.065 ( 8.065)\tPSNR 22.27 (22.27)\n",
      "* PSNR: 21.52.\n",
      "\n",
      "Valid: [0/4]\tTime  8.450 ( 8.450)\tPSNR 22.26 (22.26)\n",
      "* PSNR: 21.51.\n",
      "\n",
      "Valid: [0/4]\tTime  8.252 ( 8.252)\tPSNR 22.25 (22.25)\n",
      "* PSNR: 21.51.\n",
      "\n",
      "Valid: [0/4]\tTime  7.663 ( 7.663)\tPSNR 22.27 (22.27)\n",
      "* PSNR: 21.53.\n",
      "\n",
      "Valid: [0/4]\tTime  8.126 ( 8.126)\tPSNR 22.26 (22.26)\n",
      "* PSNR: 21.51.\n",
      "\n",
      "Valid: [0/4]\tTime  8.218 ( 8.218)\tPSNR 22.28 (22.28)\n",
      "* PSNR: 21.53.\n",
      "\n",
      "Valid: [0/4]\tTime  8.137 ( 8.137)\tPSNR 22.21 (22.21)\n",
      "* PSNR: 21.46.\n",
      "\n",
      "Valid: [0/4]\tTime  8.161 ( 8.161)\tPSNR 22.26 (22.26)\n",
      "* PSNR: 21.52.\n",
      "\n",
      "Valid: [0/4]\tTime  7.523 ( 7.523)\tPSNR 22.24 (22.24)\n",
      "* PSNR: 21.50.\n",
      "\n",
      "Valid: [0/4]\tTime  8.152 ( 8.152)\tPSNR 22.23 (22.23)\n",
      "* PSNR: 21.48.\n",
      "\n",
      "Valid: [0/4]\tTime  8.313 ( 8.313)\tPSNR 22.31 (22.31)\n",
      "* PSNR: 21.56.\n",
      "\n",
      "Valid: [0/4]\tTime  8.159 ( 8.159)\tPSNR 22.30 (22.30)\n",
      "* PSNR: 21.55.\n",
      "\n",
      "Valid: [0/4]\tTime  8.679 ( 8.679)\tPSNR 22.16 (22.16)\n",
      "* PSNR: 21.43.\n",
      "\n",
      "Valid: [0/4]\tTime  7.597 ( 7.597)\tPSNR 22.28 (22.28)\n",
      "* PSNR: 21.53.\n",
      "\n",
      "Valid: [0/4]\tTime  8.919 ( 8.919)\tPSNR 22.29 (22.29)\n",
      "* PSNR: 21.54.\n",
      "\n",
      "Valid: [0/4]\tTime  7.636 ( 7.636)\tPSNR 22.28 (22.28)\n",
      "* PSNR: 21.54.\n",
      "\n",
      "Valid: [0/4]\tTime  8.557 ( 8.557)\tPSNR 22.27 (22.27)\n",
      "* PSNR: 21.52.\n",
      "\n",
      "Valid: [0/4]\tTime  8.261 ( 8.261)\tPSNR 22.31 (22.31)\n",
      "* PSNR: 21.56.\n",
      "\n",
      "Valid: [0/4]\tTime  8.344 ( 8.344)\tPSNR 22.25 (22.25)\n",
      "* PSNR: 21.51.\n",
      "\n",
      "Valid: [0/4]\tTime  7.968 ( 7.968)\tPSNR 22.30 (22.30)\n",
      "* PSNR: 21.56.\n",
      "\n",
      "Valid: [0/4]\tTime  8.194 ( 8.194)\tPSNR 22.32 (22.32)\n",
      "* PSNR: 21.58.\n",
      "\n",
      "Valid: [0/4]\tTime  8.165 ( 8.165)\tPSNR 22.27 (22.27)\n",
      "* PSNR: 21.53.\n",
      "\n",
      "Valid: [0/4]\tTime  8.215 ( 8.215)\tPSNR 22.34 (22.34)\n",
      "* PSNR: 21.59.\n",
      "\n",
      "Valid: [0/4]\tTime  8.208 ( 8.208)\tPSNR 22.39 (22.39)\n",
      "* PSNR: 21.63.\n",
      "\n",
      "Valid: [0/4]\tTime  8.263 ( 8.263)\tPSNR 22.38 (22.38)\n",
      "* PSNR: 21.63.\n",
      "\n",
      "Valid: [0/4]\tTime  7.850 ( 7.850)\tPSNR 22.30 (22.30)\n",
      "* PSNR: 21.55.\n",
      "\n",
      "Valid: [0/4]\tTime  8.071 ( 8.071)\tPSNR 22.37 (22.37)\n",
      "* PSNR: 21.62.\n",
      "\n",
      "Valid: [0/4]\tTime  8.398 ( 8.398)\tPSNR 22.41 (22.41)\n",
      "* PSNR: 21.66.\n",
      "\n",
      "Valid: [0/4]\tTime  8.274 ( 8.274)\tPSNR 22.34 (22.34)\n",
      "* PSNR: 21.59.\n",
      "\n",
      "Valid: [0/4]\tTime  8.542 ( 8.542)\tPSNR 22.35 (22.35)\n",
      "* PSNR: 21.60.\n",
      "\n",
      "Valid: [0/4]\tTime  7.836 ( 7.836)\tPSNR 22.39 (22.39)\n",
      "* PSNR: 21.64.\n",
      "\n",
      "Valid: [0/4]\tTime  8.917 ( 8.917)\tPSNR 22.33 (22.33)\n",
      "* PSNR: 21.58.\n",
      "\n",
      "Valid: [0/4]\tTime  7.955 ( 7.955)\tPSNR 22.37 (22.37)\n",
      "* PSNR: 21.61.\n",
      "\n",
      "Valid: [0/4]\tTime  8.809 ( 8.809)\tPSNR 22.36 (22.36)\n",
      "* PSNR: 21.61.\n",
      "\n",
      "Valid: [0/4]\tTime  8.184 ( 8.184)\tPSNR 22.40 (22.40)\n",
      "* PSNR: 21.65.\n",
      "\n",
      "Valid: [0/4]\tTime  8.343 ( 8.343)\tPSNR 22.41 (22.41)\n",
      "* PSNR: 21.65.\n",
      "\n",
      "Valid: [0/4]\tTime  8.288 ( 8.288)\tPSNR 22.39 (22.39)\n",
      "* PSNR: 21.64.\n",
      "\n",
      "Valid: [0/4]\tTime  8.842 ( 8.842)\tPSNR 22.39 (22.39)\n",
      "* PSNR: 21.63.\n",
      "\n",
      "Valid: [0/4]\tTime  8.065 ( 8.065)\tPSNR 22.40 (22.40)\n",
      "* PSNR: 21.64.\n",
      "\n",
      "Valid: [0/4]\tTime  8.755 ( 8.755)\tPSNR 22.37 (22.37)\n",
      "* PSNR: 21.62.\n",
      "\n",
      "Valid: [0/4]\tTime  7.707 ( 7.707)\tPSNR 22.38 (22.38)\n",
      "* PSNR: 21.63.\n",
      "\n",
      "Valid: [0/4]\tTime  8.727 ( 8.727)\tPSNR 22.39 (22.39)\n",
      "* PSNR: 21.63.\n",
      "\n",
      "Valid: [0/4]\tTime  8.295 ( 8.295)\tPSNR 22.37 (22.37)\n",
      "* PSNR: 21.62.\n",
      "\n",
      "Valid: [0/4]\tTime  7.985 ( 7.985)\tPSNR 22.42 (22.42)\n",
      "* PSNR: 21.66.\n",
      "\n",
      "Valid: [0/4]\tTime  7.820 ( 7.820)\tPSNR 22.41 (22.41)\n",
      "* PSNR: 21.65.\n",
      "\n",
      "Valid: [0/4]\tTime  7.521 ( 7.521)\tPSNR 22.41 (22.41)\n",
      "* PSNR: 21.65.\n",
      "\n",
      "Valid: [0/4]\tTime  7.599 ( 7.599)\tPSNR 22.37 (22.37)\n",
      "* PSNR: 21.62.\n",
      "\n",
      "Valid: [0/4]\tTime  7.335 ( 7.335)\tPSNR 22.36 (22.36)\n",
      "* PSNR: 21.61.\n",
      "\n",
      "Valid: [0/4]\tTime  7.465 ( 7.465)\tPSNR 22.41 (22.41)\n",
      "* PSNR: 21.66.\n",
      "\n",
      "Valid: [0/4]\tTime  7.892 ( 7.892)\tPSNR 22.35 (22.35)\n",
      "* PSNR: 21.60.\n",
      "\n",
      "Valid: [0/4]\tTime  7.553 ( 7.553)\tPSNR 22.40 (22.40)\n",
      "* PSNR: 21.64.\n",
      "\n",
      "Valid: [0/4]\tTime  8.131 ( 8.131)\tPSNR 22.35 (22.35)\n",
      "* PSNR: 21.60.\n",
      "\n",
      "End train ESRGAN model.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"Load train dataset and valid dataset...\")\n",
    "    train_dataloader, valid_dataloader = load_dataset()\n",
    "    print(\"Load train dataset and valid dataset successfully.\")\n",
    "\n",
    "    print(\"Build ESRGAN model...\")\n",
    "    discriminator, generator = build_model()\n",
    "    print(\"Build ESRGAN model successfully.\")\n",
    "\n",
    "    print(\"Define all loss functions...\")\n",
    "    psnr_criterion, pixel_criterion, content_criterion, adversarial_criterion = define_loss()\n",
    "    print(\"Define all loss functions successfully.\")\n",
    "\n",
    "    print(\"Define all optimizer functions...\")\n",
    "    d_optimizer, g_optimizer = define_optimizer(discriminator, generator)\n",
    "    print(\"Define all optimizer functions successfully.\")\n",
    "\n",
    "    print(\"Define all optimizer scheduler...\")\n",
    "    d_scheduler, g_scheduler = define_scheduler(d_optimizer, g_optimizer)\n",
    "    print(\"Define all optimizer scheduler functions successfully.\")\n",
    "\n",
    "    print(\"Check whether the training weight is restored...\")\n",
    "    resume_checkpoint(discriminator, generator)\n",
    "    print(\"Check whether the training weight is restored successfully.\")\n",
    "\n",
    "    # Create a folder of super-resolution experiment results\n",
    "    samples_dir = os.path.join(\"samples\", exp_name)\n",
    "    results_dir = os.path.join(\"results\", exp_name)\n",
    "    if not os.path.exists(samples_dir):\n",
    "        os.makedirs(samples_dir)\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    # Create training process log file\n",
    "    writer = SummaryWriter(os.path.join(\"samples\", \"logs\", exp_name))\n",
    "\n",
    "    # Initialize the gradient scaler.\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    # Initialize training to generate network evaluation indicators\n",
    "    best_psnr = 0.0\n",
    "\n",
    "    print(\"Start train ESRGAN model.\")\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        train(discriminator,\n",
    "              generator,\n",
    "              train_dataloader,\n",
    "              psnr_criterion,\n",
    "              pixel_criterion,\n",
    "              content_criterion,\n",
    "              adversarial_criterion,\n",
    "              d_optimizer,\n",
    "              g_optimizer,\n",
    "              epoch,\n",
    "              scaler,\n",
    "              writer)\n",
    "\n",
    "        psnr = validate(generator, valid_dataloader, psnr_criterion, epoch, writer)\n",
    "        # Automatically save the model with the highest index\n",
    "        is_best = psnr > best_psnr\n",
    "        best_psnr = max(psnr, best_psnr)\n",
    "        torch.save(discriminator.state_dict(), os.path.join(samples_dir, f\"d_epoch_{epoch + 1}.pth\"))\n",
    "        torch.save(generator.state_dict(), os.path.join(samples_dir, f\"g_epoch_{epoch + 1}.pth\"))\n",
    "        if is_best:\n",
    "            torch.save(discriminator.state_dict(), os.path.join(results_dir, \"d-best.pth\"))\n",
    "            torch.save(generator.state_dict(), os.path.join(results_dir, f\"g-best.pth\"))\n",
    "\n",
    "        # Update LR\n",
    "        d_scheduler.step()\n",
    "        g_scheduler.step()\n",
    "\n",
    "    # Save the generator weight under the last Epoch in this stage\n",
    "    torch.save(discriminator.state_dict(), os.path.join(results_dir, \"d-last.pth\"))\n",
    "    torch.save(generator.state_dict(), os.path.join(results_dir, \"g-last.pth\"))\n",
    "    print(\"End train ESRGAN model.\")\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    # Initialize the LMDB data set class and write the contents of the LMDB database file into memory\n",
    "    train_datasets = ImageDataset(train_image_dir, image_size, upscale_factor, \"train\")\n",
    "    valid_datasets = ImageDataset(valid_image_dir, image_size, upscale_factor, \"valid\")\n",
    "    # Make it into a data set type supported by PyTorch\n",
    "    train_dataloader = DataLoader(train_datasets,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  persistent_workers=True)\n",
    "    valid_dataloader = DataLoader(valid_datasets,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  persistent_workers=True)\n",
    "\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "\n",
    "def build_model() -> nn.Module:\n",
    "    discriminator = Discriminator().to(device)\n",
    "    generator = Generator().to(device)\n",
    "\n",
    "    return discriminator, generator\n",
    "\n",
    "\n",
    "def define_loss():\n",
    "    psnr_criterion = nn.MSELoss().to(device)\n",
    "    pixel_criterion = nn.L1Loss().to(device)\n",
    "    content_criterion = ContentLoss().to(device)\n",
    "    adversarial_criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "    return psnr_criterion, pixel_criterion, content_criterion, adversarial_criterion\n",
    "\n",
    "\n",
    "def define_optimizer(discriminator: nn.Module, generator: nn.Module):\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), d_model_lr, d_model_betas)\n",
    "    g_optimizer = optim.Adam(generator.parameters(), g_model_lr, g_model_betas)\n",
    "\n",
    "    return d_optimizer, g_optimizer\n",
    "\n",
    "\n",
    "def define_scheduler(d_optimizer: optim.Adam, g_optimizer: optim.Adam):\n",
    "    d_scheduler = lr_scheduler.MultiStepLR(d_optimizer, d_optimizer_milestones, d_optimizer_gamma)\n",
    "    g_scheduler = lr_scheduler.MultiStepLR(g_optimizer, g_optimizer_milestones, g_optimizer_gamma)\n",
    "\n",
    "    return d_scheduler, g_scheduler\n",
    "\n",
    "\n",
    "def resume_checkpoint(discriminator: nn.Module, generator: nn.Module) -> None:\n",
    "    if resume:\n",
    "        if resume_d_weight != \"\":\n",
    "            # Get pretrained model state dict\n",
    "            pretrained_state_dict = torch.load(resume_d_weight)\n",
    "            model_state_dict = discriminator.state_dict()\n",
    "            # Extract the fitted model weights\n",
    "            new_state_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict.items()}\n",
    "            # Overwrite the pretrained model weights to the current model\n",
    "            model_state_dict.update(new_state_dict)\n",
    "            discriminator.load_state_dict(model_state_dict, strict=strict)\n",
    "        if resume_g_weight != \"\":\n",
    "            # Get pretrained model state dict\n",
    "            pretrained_state_dict = torch.load(resume_g_weight)\n",
    "            model_state_dict = generator.state_dict()\n",
    "            # Extract the fitted model weights\n",
    "            new_state_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict.items()}\n",
    "            # Overwrite the pretrained model weights to the current model\n",
    "            model_state_dict.update(new_state_dict)\n",
    "            generator.load_state_dict(model_state_dict, strict=strict)\n",
    "\n",
    "\n",
    "def train(discriminator,\n",
    "          generator,\n",
    "          train_dataloader,\n",
    "          psnr_criterion,\n",
    "          pixel_criterion,\n",
    "          content_criterion,\n",
    "          adversarial_criterion,\n",
    "          d_optimizer,\n",
    "          g_optimizer,\n",
    "          epoch,\n",
    "          scaler,\n",
    "          writer) -> None:\n",
    "    # Calculate how many iterations there are under epoch\n",
    "    batches = len(train_dataloader)\n",
    "\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    data_time = AverageMeter(\"Data\", \":6.3f\")\n",
    "    pixel_losses = AverageMeter(\"Pixel loss\", \":6.6f\")\n",
    "    content_losses = AverageMeter(\"Content loss\", \":6.6f\")\n",
    "    adversarial_losses = AverageMeter(\"Adversarial loss\", \":6.6f\")\n",
    "    d_hr_probabilities = AverageMeter(\"D(HR)\", \":6.3f\")\n",
    "    d_sr_probabilities = AverageMeter(\"D(SR)\", \":6.3f\")\n",
    "    psnres = AverageMeter(\"PSNR\", \":4.2f\")\n",
    "    progress = ProgressMeter(batches,\n",
    "                             [batch_time, data_time,\n",
    "                              pixel_losses, content_losses, adversarial_losses,\n",
    "                              d_hr_probabilities, d_sr_probabilities,\n",
    "                              psnres],\n",
    "                             prefix=f\"Epoch: [{epoch + 1}]\")\n",
    "\n",
    "    # Put all model in train mode.\n",
    "    discriminator.train()\n",
    "    generator.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for index, (lr, hr) in enumerate(train_dataloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # Send data to designated device\n",
    "        lr = lr.to(device, non_blocking=True)\n",
    "        hr = hr.to(device, non_blocking=True)\n",
    "\n",
    "        # Set the real sample label to 1, and the false sample label to 0\n",
    "        real_label = torch.full([lr.size(0), 1], 1.0, dtype=lr.dtype, device=device)\n",
    "        fake_label = torch.full([lr.size(0), 1], 0.0, dtype=lr.dtype, device=device)\n",
    "\n",
    "        # Use generators to create super-resolution images\n",
    "        sr = generator(lr)\n",
    "\n",
    "        # Start training discriminator\n",
    "        # At this stage, the discriminator needs to require a derivative gradient\n",
    "        for p in discriminator.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # Initialize the discriminator optimizer gradient\n",
    "        d_optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the loss of the discriminator on the high-resolution image\n",
    "        with amp.autocast():\n",
    "            hr_output = discriminator(hr)\n",
    "            sr_output = discriminator(sr.detach())\n",
    "            d_loss_hr = adversarial_criterion(hr_output - torch.mean(sr_output), real_label)\n",
    "            d_loss_sr = adversarial_criterion(sr_output - torch.mean(hr_output), fake_label)\n",
    "        # Gradient zoom\n",
    "        scaler.scale(d_loss_hr).backward(retain_graph=True)\n",
    "        scaler.scale(d_loss_sr).backward()\n",
    "\n",
    "        # Update gradient\n",
    "        scaler.step(d_optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Count discriminator total loss\n",
    "        d_loss = d_loss_hr + d_loss_sr\n",
    "        # End training discriminator\n",
    "\n",
    "        # Start training generator\n",
    "        # At this stage, the discriminator no needs to require a derivative gradient\n",
    "        for p in discriminator.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Initialize the generator optimizer gradient\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the loss of the generator on the super-resolution image\n",
    "        with amp.autocast():\n",
    "            hr_output = discriminator(hr.detach())\n",
    "            sr_output = discriminator(sr)\n",
    "            pixel_loss = pixel_weight * pixel_criterion(sr, hr.detach())\n",
    "            content_loss = content_weight * content_criterion(sr, hr.detach())\n",
    "            adversarial_loss = adversarial_weight * adversarial_criterion(sr_output - torch.mean(hr_output), real_label)\n",
    "        # Count discriminator total loss\n",
    "        g_loss = pixel_loss + content_loss + adversarial_loss\n",
    "        # Gradient zoom\n",
    "        scaler.scale(g_loss).backward()\n",
    "        # Update generator parameters\n",
    "        scaler.step(g_optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # End training generator\n",
    "\n",
    "        # Calculate the scores of the two images on the discriminator\n",
    "        d_hr_probability = torch.sigmoid(torch.mean(hr_output))\n",
    "        d_sr_probability = torch.sigmoid(torch.mean(sr_output))\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        psnr = 10. * torch.log10(1. / psnr_criterion(sr, hr))\n",
    "        pixel_losses.update(pixel_loss.item(), lr.size(0))\n",
    "        content_losses.update(content_loss.item(), lr.size(0))\n",
    "        adversarial_losses.update(adversarial_loss.item(), lr.size(0))\n",
    "        d_hr_probabilities.update(d_hr_probability.item(), lr.size(0))\n",
    "        d_sr_probabilities.update(d_sr_probability.item(), lr.size(0))\n",
    "        psnres.update(psnr.item(), lr.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        iters = index + epoch * batches + 1\n",
    "        writer.add_scalar(\"Train/D_Loss\", d_loss.item(), iters)\n",
    "        writer.add_scalar(\"Train/G_Loss\", g_loss.item(), iters)\n",
    "        writer.add_scalar(\"Train/Pixel_Loss\", pixel_loss.item(), iters)\n",
    "        writer.add_scalar(\"Train/Content_Loss\", content_loss.item(), iters)\n",
    "        writer.add_scalar(\"Train/Adversarial_Loss\", adversarial_loss.item(), iters)\n",
    "        writer.add_scalar(\"Train/D(HR)_Probability\", d_hr_probability.item(), iters)\n",
    "        writer.add_scalar(\"Train/D(SR)_Probability\", d_sr_probability.item(), iters)\n",
    "        if index % print_frequency == 0 and index != 0:\n",
    "            progress.display(index)\n",
    "\n",
    "\n",
    "def validate(model, valid_dataloader, psnr_criterion, epoch, writer) -> float:\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    psnres = AverageMeter(\"PSNR\", \":4.2f\")\n",
    "    progress = ProgressMeter(len(valid_dataloader), [batch_time, psnres], prefix=\"Valid: \")\n",
    "\n",
    "    # Put the generator in verification mode.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for index, (lr, hr) in enumerate(valid_dataloader):\n",
    "            lr = lr.to(device, non_blocking=True)\n",
    "            hr = hr.to(device, non_blocking=True)\n",
    "\n",
    "            # Mixed precision\n",
    "            with amp.autocast():\n",
    "                sr = model(lr)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            psnr = 10. * torch.log10(1. / psnr_criterion(sr, hr))\n",
    "            psnres.update(psnr.item(), hr.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if index % print_frequency == 0:\n",
    "                progress.display(index)\n",
    "\n",
    "        writer.add_scalar(\"Valid/PSNR\", psnres.avg, epoch + 1)\n",
    "        # Print evaluation indicators.\n",
    "        print(f\"* PSNR: {psnres.avg:4.2f}.\\n\")\n",
    "\n",
    "    return psnres.avg\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=\":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print(\"\\t\".join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
    "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6693e16e",
   "metadata": {
    "papermill": {
     "duration": 0.196151,
     "end_time": "2022-02-17T23:12:18.574166",
     "exception": false,
     "start_time": "2022-02-17T23:12:18.378015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19936.51339,
   "end_time": "2022-02-17T23:12:22.553404",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-17T17:40:06.040014",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2897e47a1c204361ae9b3bcf85cfb4cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ec868132470e4146b4263acc23439722",
       "max": 574673361,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3139060fc92447cc88bd82c95feae74c",
       "value": 574673361
      }
     },
     "3139060fc92447cc88bd82c95feae74c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3e5629c357b24f4b886ce62b04631899": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f36455f94ad406fa70a4519bb4c49ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75c62842b3af41fa9d4efb25028c8114": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a94757e4ed994068b3728f9f84c340b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b5376be2bb904acb855ed5cd577c1132": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3e5629c357b24f4b886ce62b04631899",
       "placeholder": "​",
       "style": "IPY_MODEL_a94757e4ed994068b3728f9f84c340b2",
       "value": " 548M/548M [00:07&lt;00:00, 77.5MB/s]"
      }
     },
     "c6471ab88108472780ccc16f2f9bf414": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e8e3c234361c4217850afb6510df205b",
        "IPY_MODEL_2897e47a1c204361ae9b3bcf85cfb4cd",
        "IPY_MODEL_b5376be2bb904acb855ed5cd577c1132"
       ],
       "layout": "IPY_MODEL_5f36455f94ad406fa70a4519bb4c49ed"
      }
     },
     "e8e3c234361c4217850afb6510df205b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ed6d0c34ae3e449d989721d0fe9f0ddd",
       "placeholder": "​",
       "style": "IPY_MODEL_75c62842b3af41fa9d4efb25028c8114",
       "value": "100%"
      }
     },
     "ec868132470e4146b4263acc23439722": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ed6d0c34ae3e449d989721d0fe9f0ddd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
